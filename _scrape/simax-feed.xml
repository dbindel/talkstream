<?xml version="1.0" encoding="UTF-8"?>
<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:dc="http://purl.org/dc/elements/1.1/"
         xmlns:content="http://purl.org/rss/1.0/modules/content/"
         xmlns:rss="http://purl.org/rss/1.0/"
         xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/">
   <rss:channel rdf:about="http://epubs.siam.org/loi/sjmael?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Society for Industrial and Applied Mathematics: SIAM Journal on Matrix Analysis and Applications: Table of Contents</rss:title>
      <rss:description>Table of Contents for SIAM Journal on Matrix Analysis and Applications. List of articles from both the latest and ahead of print issues.</rss:description>
      <rss:link>http://epubs.siam.org/loi/sjmael?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <dc:title>Society for Industrial and Applied Mathematics: SIAM Journal on Matrix Analysis and Applications: Table of Contents</dc:title>
      <dc:publisher>Society for Industrial and Applied Mathematics</dc:publisher>
      <dc:language>en-US</dc:language>
      <prism:publicationName>SIAM Journal on Matrix Analysis and Applications</prism:publicationName>
      <rss:items>
         <rdf:Seq>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/13092232X?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130927176?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/140961389?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130931734?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130943613?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130916394?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130945156?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130929060?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130927462?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/090771430?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130926171?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/13093858X?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/110825753?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/120896499?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130914966?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130943455?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130945259?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130945995?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130938207?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/120869286?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130917260?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130940414?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130909949?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130935112?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130931655?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/140953150?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/120894294?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130934933?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/13090866X?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130907811?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/110858148?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130915339?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130925621?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/120898784?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130921234?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130920897?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/120897687?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130911032?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130935665?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130916710?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130912372?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130933228?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130916084?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/13093491X?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130927231?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130933472?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/120902677?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130935537?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130946563?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130920629?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130912438?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/120893057?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/120895755?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/120892891?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130920137?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/120889848?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130919490?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130911962?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130922872?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/12088906X?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130905010?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130905216?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/120895639?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/120869432?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
            <rdf:li rdf:resource="http://epubs.siam.org/doi/abs/10.1137/130944552?ai=s7&amp;mi=3ezuvv&amp;af=R"/>
         </rdf:Seq>
      </rss:items>
   </rss:channel>
   <rss:image rdf:about="http://epubs.siam.org/na101/home/literatum/publisher/siam/journals/covergifs/sjmael/cover.png">
      <rss:title>SIAM Journal on Matrix Analysis and Applications</rss:title>
      <rss:url>http://epubs.siam.org/na101/home/literatum/publisher/siam/journals/covergifs/sjmael/cover.png</rss:url>
      <rss:link>http://epubs.siam.org/loi/sjmael?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
   </rss:image>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/13092232X?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Nonnegative Tensor Factorization, Completely Positive Tensors, and a Hierarchical Elimination Algorithm</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/13092232X?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/4"&gt;Volume 35, Issue 4&lt;/a&gt;, Page 1227-1241, January 2014. &lt;br/&gt; Nonnegative tensor factorization has applications in statistics, computer vision, exploratory multiway data analysis, and blind source separation. A symmetric nonnegative tensor, which has an exact symmetric nonnegative factorization, is called a completely positive tensor.  This concept extends the concept of completely positive matrices.    A classical result in the theory of completely positive matrices is that a symmetric, diagonally dominated nonnegative matrix is a completely positive matrix.   In this paper, we introduce strongly symmetric tensors and show that a symmetric tensor has a symmetric binary decomposition if and only if it is strongly symmetric.  Then we show that a strongly symmetric, hierarchically dominated nonnegative tensor is a completely positive tensor, and present a hierarchical elimination algorithm for checking this. Numerical examples are given to illustrate this. Some other properties of completely positive tensors are discussed. In particular, we show that the completely positive tensor cone and the co-positive tensor cone of the same order are dual to each other.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 4, Page 1227-1241, January 2014. &lt;br/&gt; Nonnegative tensor factorization has applications in statistics, computer vision, exploratory multiway data analysis, and blind source separation. A symmetric nonnegative tensor, which has an exact symmetric nonnegative factorization, is called a completely positive tensor.  This concept extends the concept of completely positive matrices.    A classical result in the theory of completely positive matrices is that a symmetric, diagonally dominated nonnegative matrix is a completely positive matrix.   In this paper, we introduce strongly symmetric tensors and show that a symmetric tensor has a symmetric binary decomposition if and only if it is strongly symmetric.  Then we show that a strongly symmetric, hierarchically dominated nonnegative tensor is a completely positive tensor, and present a hierarchical elimination algorithm for checking this. Numerical examples are given to illustrate this. Some other properties of completely positive tensors are discussed. In particular, we show that the completely positive tensor cone and the co-positive tensor cone of the same order are dual to each other.</rss:description>
      <dc:title>Nonnegative Tensor Factorization, Completely Positive Tensors, and a Hierarchical Elimination Algorithm</dc:title>
      <dc:identifier>doi:10.1137/13092232X</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 09 Oct 2014 07:00:00 GMT</dc:date>Liqun Qi, Changqing Xu, and Yi Xu<prism:publicationName>Nonnegative Tensor Factorization, Completely Positive Tensors, and a Hierarchical Elimination Algorithm</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>4</prism:number>
      <prism:startingPage>1227</prism:startingPage>
      <prism:endingPage>1241</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/13092232X</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/13092232X?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130927176?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>The Canonical Decomposition of $\mathcal{C}^n_d$ and Numerical Gröbner and Border Bases</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130927176?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/4"&gt;Volume 35, Issue 4&lt;/a&gt;, Page 1242-1264, January 2014. &lt;br/&gt; This article introduces the canonical decomposition of the vector space of multivariate polynomials for a given monomial ordering. Its importance lies in solving multivariate polynomial systems, computing Gröbner bases, and solving the ideal membership problem. An SVD-based algorithm is presented that numerically computes the canonical decomposition. It is then shown how, by introducing the notion of divisibility into this algorithm, a numerical Gröbner basis can also be computed. In addition, we demonstrate how the canonical decomposition can be used to decide whether the affine solution set of a multivariate polynomial system is zero-dimensional and to solve the ideal membership problem numerically. The SVD-based canonical decomposition algorithm is also extended to numerically compute border bases. A tolerance for each of the algorithms is derived using perturbation theory of principal angles. This derivation shows that the condition number of computing the canonical decomposition and numerical Gröbner basis is essentially the condition number of the Macaulay matrix. Numerical experiments with both exact and noisy coefficients are presented and discussed.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 4, Page 1242-1264, January 2014. &lt;br/&gt; This article introduces the canonical decomposition of the vector space of multivariate polynomials for a given monomial ordering. Its importance lies in solving multivariate polynomial systems, computing Gröbner bases, and solving the ideal membership problem. An SVD-based algorithm is presented that numerically computes the canonical decomposition. It is then shown how, by introducing the notion of divisibility into this algorithm, a numerical Gröbner basis can also be computed. In addition, we demonstrate how the canonical decomposition can be used to decide whether the affine solution set of a multivariate polynomial system is zero-dimensional and to solve the ideal membership problem numerically. The SVD-based canonical decomposition algorithm is also extended to numerically compute border bases. A tolerance for each of the algorithms is derived using perturbation theory of principal angles. This derivation shows that the condition number of computing the canonical decomposition and numerical Gröbner basis is essentially the condition number of the Macaulay matrix. Numerical experiments with both exact and noisy coefficients are presented and discussed.</rss:description>
      <dc:title>The Canonical Decomposition of $\mathcal{C}^n_d$ and Numerical Gröbner and Border Bases</dc:title>
      <dc:identifier>doi:10.1137/130927176</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 09 Oct 2014 07:00:00 GMT</dc:date>Kim Batselier, Philippe Dreesen, and Bart De Moor<prism:publicationName>The Canonical Decomposition of $\mathcal{C}^n_d$ and Numerical Gröbner and Border Bases</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>4</prism:number>
      <prism:startingPage>1242</prism:startingPage>
      <prism:endingPage>1264</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130927176</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130927176?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/140961389?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>An Algorithm For Generic and Low-Rank Specific Identifiability of Complex Tensors</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/140961389?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/4"&gt;Volume 35, Issue 4&lt;/a&gt;, Page 1265-1287, January 2014. &lt;br/&gt; We propose a new sufficient condition for verifying whether general rank-$r$ complex tensors of arbitrary order admit a unique decomposition as a linear combination of rank-$1$ tensors. A practical algorithm is proposed for verifying this condition, with which it was established that in all spaces of dimension less than 15000, with a few known exceptions,  listed in the paper, generic identifiability holds for ranks up to one less than the generic rank of the space. This is the largest possible rank value for which generic identifiability can hold, except for spaces with a perfect shape. The algorithm can also verify the identifiability of a given specific rank-$r$ decomposition, provided that it can be shown to correspond to a nonsingular point of the $r$th order secant variety. For sufficiently small rank, which nevertheless improves upon the known bounds for specific identifiability, some local equations of this variety are known, allowing us to verify this property. As a particular example of our approach, we prove the identifiability of a specific $5\times 5\times 5$ tensor of rank $7$, which cannot be handled by the conditions recently provided in [I. Domanov and L. De Lathauwer, SIAM J. Matrix Anal. Appl., 34 (2013), pp. 876--903]. Finally, we also present a surprising new class of weakly defective Segre varieties that nevertheless turns out to admit a generically unique decomposition.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 4, Page 1265-1287, January 2014. &lt;br/&gt; We propose a new sufficient condition for verifying whether general rank-$r$ complex tensors of arbitrary order admit a unique decomposition as a linear combination of rank-$1$ tensors. A practical algorithm is proposed for verifying this condition, with which it was established that in all spaces of dimension less than 15000, with a few known exceptions,  listed in the paper, generic identifiability holds for ranks up to one less than the generic rank of the space. This is the largest possible rank value for which generic identifiability can hold, except for spaces with a perfect shape. The algorithm can also verify the identifiability of a given specific rank-$r$ decomposition, provided that it can be shown to correspond to a nonsingular point of the $r$th order secant variety. For sufficiently small rank, which nevertheless improves upon the known bounds for specific identifiability, some local equations of this variety are known, allowing us to verify this property. As a particular example of our approach, we prove the identifiability of a specific $5\times 5\times 5$ tensor of rank $7$, which cannot be handled by the conditions recently provided in [I. Domanov and L. De Lathauwer, SIAM J. Matrix Anal. Appl., 34 (2013), pp. 876--903]. Finally, we also present a surprising new class of weakly defective Segre varieties that nevertheless turns out to admit a generically unique decomposition.</rss:description>
      <dc:title>An Algorithm For Generic and Low-Rank Specific Identifiability of Complex Tensors</dc:title>
      <dc:identifier>doi:10.1137/140961389</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 23 Oct 2014 07:00:00 GMT</dc:date>Luca Chiantini, Giorgio Ottaviani, and Nick Vannieuwenhoven<prism:publicationName>An Algorithm For Generic and Low-Rank Specific Identifiability of Complex Tensors</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>4</prism:number>
      <prism:startingPage>1265</prism:startingPage>
      <prism:endingPage>1287</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/140961389</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/140961389?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130931734?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>On the Decay of the Elements of Inverse Triangular Toeplitz Matrices</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130931734?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/4"&gt;Volume 35, Issue 4&lt;/a&gt;, Page 1288-1302, January 2014. &lt;br/&gt; We consider half-infinite triangular Toeplitz matrices with slow decay of the elements and prove under a monotonicity condition that the elements of the inverse matrix, as well as the elements of the fundamental matrix, decay to zero. We provide a quantitative description of the decay of the fundamental matrix in terms of $p$-norms. The results add to the classical results of Jaffard and Vecchio and are illustrated by numerical examples.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 4, Page 1288-1302, January 2014. &lt;br/&gt; We consider half-infinite triangular Toeplitz matrices with slow decay of the elements and prove under a monotonicity condition that the elements of the inverse matrix, as well as the elements of the fundamental matrix, decay to zero. We provide a quantitative description of the decay of the fundamental matrix in terms of $p$-norms. The results add to the classical results of Jaffard and Vecchio and are illustrated by numerical examples.</rss:description>
      <dc:title>On the Decay of the Elements of Inverse Triangular Toeplitz Matrices</dc:title>
      <dc:identifier>doi:10.1137/130931734</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Tue, 28 Oct 2014 07:00:00 GMT</dc:date>Neville J. Ford, Dmitry V. Savostyanov, and Nickolai L. Zamarashkin<prism:publicationName>On the Decay of the Elements of Inverse Triangular Toeplitz Matrices</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>4</prism:number>
      <prism:startingPage>1288</prism:startingPage>
      <prism:endingPage>1302</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130931734</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130931734?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130943613?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Relative Perturbation Theory for Diagonally Dominant Matrices</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130943613?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/4"&gt;Volume 35, Issue 4&lt;/a&gt;, Page 1303-1328, January 2014. &lt;br/&gt; In this paper, strong relative perturbation bounds are developed for a number of  linear algebra problems involving diagonally dominant matrices. The key point is to parameterize diagonally dominant matrices using their off-diagonal entries and diagonally dominant parts and to consider small relative componentwise perturbations of these parameters. This allows us to obtain new relative perturbation bounds for the inverse, the solution to linear systems, the symmetric indefinite eigenvalue problem, the singular value problem, and the nonsymmetric eigenvalue problem. These bounds are much stronger than traditional perturbation results, since they are independent of either the standard condition number or the magnitude of eigenvalues/singular values. Together with previously derived perturbation bounds for the LDU factorization and the symmetric positive definite eigenvalue problem, this paper presents a complete and detailed account of relative structured perturbation theory for diagonally dominant matrices.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 4, Page 1303-1328, January 2014. &lt;br/&gt; In this paper, strong relative perturbation bounds are developed for a number of  linear algebra problems involving diagonally dominant matrices. The key point is to parameterize diagonally dominant matrices using their off-diagonal entries and diagonally dominant parts and to consider small relative componentwise perturbations of these parameters. This allows us to obtain new relative perturbation bounds for the inverse, the solution to linear systems, the symmetric indefinite eigenvalue problem, the singular value problem, and the nonsymmetric eigenvalue problem. These bounds are much stronger than traditional perturbation results, since they are independent of either the standard condition number or the magnitude of eigenvalues/singular values. Together with previously derived perturbation bounds for the LDU factorization and the symmetric positive definite eigenvalue problem, this paper presents a complete and detailed account of relative structured perturbation theory for diagonally dominant matrices.</rss:description>
      <dc:title>Relative Perturbation Theory for Diagonally Dominant Matrices</dc:title>
      <dc:identifier>doi:10.1137/130943613</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Tue, 28 Oct 2014 07:00:00 GMT</dc:date>Megan Dailey, Froilán M. Dopico, and Qiang Ye<prism:publicationName>Relative Perturbation Theory for Diagonally Dominant Matrices</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>4</prism:number>
      <prism:startingPage>1303</prism:startingPage>
      <prism:endingPage>1328</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130943613</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130943613?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130916394?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Projected Krylov Methods for Saddle-Point Systems</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130916394?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/4"&gt;Volume 35, Issue 4&lt;/a&gt;, Page 1329-1343, January 2014. &lt;br/&gt; Projected Krylov methods are full-space formulations of Krylov methods that take place in a nullspace. Provided projections into the nullspace can be computed accurately, those methods only require products between an operator and vectors lying in the nullspace. We provide systematic principles for obtaining the projected form of any well-defined Krylov method. Projected Krylov methods are mathematically equivalent to constraint-preconditioned Krylov methods provided the initial guess is well chosen, but require less memory. As a consequence, there are situations where certain known methods such as MINRES and SYMMLQ are well defined in the presence of an indefinite preconditioner.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 4, Page 1329-1343, January 2014. &lt;br/&gt; Projected Krylov methods are full-space formulations of Krylov methods that take place in a nullspace. Provided projections into the nullspace can be computed accurately, those methods only require products between an operator and vectors lying in the nullspace. We provide systematic principles for obtaining the projected form of any well-defined Krylov method. Projected Krylov methods are mathematically equivalent to constraint-preconditioned Krylov methods provided the initial guess is well chosen, but require less memory. As a consequence, there are situations where certain known methods such as MINRES and SYMMLQ are well defined in the presence of an indefinite preconditioner.</rss:description>
      <dc:title>Projected Krylov Methods for Saddle-Point Systems</dc:title>
      <dc:identifier>doi:10.1137/130916394</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Tue, 04 Nov 2014 08:00:00 GMT</dc:date>Nick Gould, Dominique Orban, and Tyrone Rees<prism:publicationName>Projected Krylov Methods for Saddle-Point Systems</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>4</prism:number>
      <prism:startingPage>1329</prism:startingPage>
      <prism:endingPage>1343</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130916394</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130916394?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130945156?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>A Moment-Matching Arnoldi Iteration for Linear Combinations of $\varphi$ Functions</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130945156?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/4"&gt;Volume 35, Issue 4&lt;/a&gt;, Page 1344-1363, January 2014. &lt;br/&gt; The action of the matrix exponential and related $\varphi$ functions on vectors plays an important role in the application of exponential integrators to ordinary differential equations. For the efficient evaluation of linear combinations of such actions we consider a new Krylov subspace algorithm. By employing Cauchy's integral formula an error representation of the numerical approximation is given. This is used to derive a priori error bounds that describe well the convergence behavior of the algorithm. Further, an efficient a posteriori estimate is constructed. Numerical experiments illustrating the convergence behavior are given in MATLAB.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 4, Page 1344-1363, January 2014. &lt;br/&gt; The action of the matrix exponential and related $\varphi$ functions on vectors plays an important role in the application of exponential integrators to ordinary differential equations. For the efficient evaluation of linear combinations of such actions we consider a new Krylov subspace algorithm. By employing Cauchy's integral formula an error representation of the numerical approximation is given. This is used to derive a priori error bounds that describe well the convergence behavior of the algorithm. Further, an efficient a posteriori estimate is constructed. Numerical experiments illustrating the convergence behavior are given in MATLAB.</rss:description>
      <dc:title>A Moment-Matching Arnoldi Iteration for Linear Combinations of $\varphi$ Functions</dc:title>
      <dc:identifier>doi:10.1137/130945156</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Tue, 04 Nov 2014 08:00:00 GMT</dc:date>Antti Koskela and Alexander Ostermann<prism:publicationName>A Moment-Matching Arnoldi Iteration for Linear Combinations of $\varphi$ Functions</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>4</prism:number>
      <prism:startingPage>1344</prism:startingPage>
      <prism:endingPage>1363</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130945156</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130945156?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130929060?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Communication-Avoiding Symmetric-Indefinite Factorization</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130929060?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/4"&gt;Volume 35, Issue 4&lt;/a&gt;, Page 1364-1406, January 2014. &lt;br/&gt; We describe and analyze a novel symmetric triangular factorization algorithm. The algorithm is essentially a block version of Aasen's triangular tridiagonalization. It factors a dense symmetric matrix $A$ as the product $A=PLTL^{T}P^{T},$ where $P$ is a permutation matrix, $L$ is lower triangular, and $T$ is block tridiagonal and banded. The algorithm is the first symmetric-indefinite communication-avoiding factorization: it performs an asymptotically optimal amount of communication in a two-level memory hierarchy for almost any cache-line size. Adaptations of the algorithm to parallel computers are likely to be communication efficient as well; one such adaptation has been recently published. The current paper describes the algorithm, proves that it is numerically stable, and proves that it is communication optimal.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 4, Page 1364-1406, January 2014. &lt;br/&gt; We describe and analyze a novel symmetric triangular factorization algorithm. The algorithm is essentially a block version of Aasen's triangular tridiagonalization. It factors a dense symmetric matrix $A$ as the product $A=PLTL^{T}P^{T},$ where $P$ is a permutation matrix, $L$ is lower triangular, and $T$ is block tridiagonal and banded. The algorithm is the first symmetric-indefinite communication-avoiding factorization: it performs an asymptotically optimal amount of communication in a two-level memory hierarchy for almost any cache-line size. Adaptations of the algorithm to parallel computers are likely to be communication efficient as well; one such adaptation has been recently published. The current paper describes the algorithm, proves that it is numerically stable, and proves that it is communication optimal.</rss:description>
      <dc:title>Communication-Avoiding Symmetric-Indefinite Factorization</dc:title>
      <dc:identifier>doi:10.1137/130929060</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 13 Nov 2014 08:00:00 GMT</dc:date>Grey Ballard, Dulceneia Becker, James Demmel, Jack Dongarra, Alex Druinsky, Inon Peled, Oded Schwartz, Sivan Toledo, and Ichitaro Yamazaki<prism:publicationName>Communication-Avoiding Symmetric-Indefinite Factorization</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>4</prism:number>
      <prism:startingPage>1364</prism:startingPage>
      <prism:endingPage>1406</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130929060</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130929060?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130927462?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Nonlinear Eigenvalue Problems with Specified Eigenvalues</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130927462?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/3"&gt;Volume 35, Issue 3&lt;/a&gt;, Page 819-834, January 2014. &lt;br/&gt; This work considers eigenvalue problems that are nonlinear in the eigenvalue parameter. Given such a nonlinear eigenvalue problem $T$, we are concerned with finding the minimal backward error such that $T$ has a set of prescribed eigenvalues with prescribed algebraic multiplicities. We consider backward errors that only allow constant perturbations, which do not depend on the eigenvalue parameter. While the usual resolvent norm addresses this question for a single eigenvalue of multiplicity one, the general setting involving several eigenvalues is significantly more difficult. Under mild assumptions, we derive a singular value optimization characterization for the minimal perturbation that addresses the general case.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 3, Page 819-834, January 2014. &lt;br/&gt; This work considers eigenvalue problems that are nonlinear in the eigenvalue parameter. Given such a nonlinear eigenvalue problem $T$, we are concerned with finding the minimal backward error such that $T$ has a set of prescribed eigenvalues with prescribed algebraic multiplicities. We consider backward errors that only allow constant perturbations, which do not depend on the eigenvalue parameter. While the usual resolvent norm addresses this question for a single eigenvalue of multiplicity one, the general setting involving several eigenvalues is significantly more difficult. Under mild assumptions, we derive a singular value optimization characterization for the minimal perturbation that addresses the general case.</rss:description>
      <dc:title>Nonlinear Eigenvalue Problems with Specified Eigenvalues</dc:title>
      <dc:identifier>doi:10.1137/130927462</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Tue, 01 Jul 2014 07:00:00 GMT</dc:date>Michael Karow, Daniel Kressner, and Emre Mengi<prism:publicationName>Nonlinear Eigenvalue Problems with Specified Eigenvalues</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>3</prism:number>
      <prism:startingPage>819</prism:startingPage>
      <prism:endingPage>834</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130927462</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130927462?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/090771430?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Nearly Linear Time Algorithms for Preconditioning and Solving Symmetric, Diagonally Dominant Linear Systems</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/090771430?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/3"&gt;Volume 35, Issue 3&lt;/a&gt;, Page 835-885, January 2014. &lt;br/&gt; We present a randomized algorithm that on input a symmetric, weakly diagonally dominant $n$-by-$n$ matrix $A$ with $m$ nonzero entries and an $n$-vector $b$ produces an ${\tilde{x}} $ such that $\|{\tilde{x}} - A^{\dagger} {b} \|_{A} \leq \epsilon \|A^{\dagger} {b}\|_{A}$ in expected time $O (m \log^{c}n \log (1/\epsilon))$ for some constant $c$. By applying this algorithm inside the inverse power method, we compute approximate Fiedler vectors in a similar amount of time. The algorithm applies subgraph preconditioners in a recursive fashion. These preconditioners improve upon the subgraph preconditioners first introduced by Vaidya in 1990. For any symmetric, weakly diagonally dominant matrix $A$ with nonpositive off-diagonal entries and $k \geq 1$, we construct in time $O (m \log^{c} n)$ a preconditioner $B$ of $A$ with at most $2 (n - 1) + O ((m/k) \log^{39} n)$ nonzero off-diagonal entries such that the finite generalized condition number $\kappa_{f} (A,B)$ is at most $k$, for some other constant $c$. In the special case when the nonzero structure of the matrix is planar  the corresponding linear system solver runs in expected time $O (n \log^{2} n + n \log n \  \log \log n \ \log (1/\epsilon))$. We hope that our introduction of algorithms of low asymptotic complexity will lead to the development of algorithms that are also fast in practice.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 3, Page 835-885, January 2014. &lt;br/&gt; We present a randomized algorithm that on input a symmetric, weakly diagonally dominant $n$-by-$n$ matrix $A$ with $m$ nonzero entries and an $n$-vector $b$ produces an ${\tilde{x}} $ such that $\|{\tilde{x}} - A^{\dagger} {b} \|_{A} \leq \epsilon \|A^{\dagger} {b}\|_{A}$ in expected time $O (m \log^{c}n \log (1/\epsilon))$ for some constant $c$. By applying this algorithm inside the inverse power method, we compute approximate Fiedler vectors in a similar amount of time. The algorithm applies subgraph preconditioners in a recursive fashion. These preconditioners improve upon the subgraph preconditioners first introduced by Vaidya in 1990. For any symmetric, weakly diagonally dominant matrix $A$ with nonpositive off-diagonal entries and $k \geq 1$, we construct in time $O (m \log^{c} n)$ a preconditioner $B$ of $A$ with at most $2 (n - 1) + O ((m/k) \log^{39} n)$ nonzero off-diagonal entries such that the finite generalized condition number $\kappa_{f} (A,B)$ is at most $k$, for some other constant $c$. In the special case when the nonzero structure of the matrix is planar  the corresponding linear system solver runs in expected time $O (n \log^{2} n + n \log n \  \log \log n \ \log (1/\epsilon))$. We hope that our introduction of algorithms of low asymptotic complexity will lead to the development of algorithms that are also fast in practice.</rss:description>
      <dc:title>Nearly Linear Time Algorithms for Preconditioning and Solving Symmetric, Diagonally Dominant Linear Systems</dc:title>
      <dc:identifier>doi:10.1137/090771430</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Tue, 08 Jul 2014 07:00:00 GMT</dc:date>Daniel A. Spielman and Shang-Hua Teng<prism:publicationName>Nearly Linear Time Algorithms for Preconditioning and Solving Symmetric, Diagonally Dominant Linear Systems</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>3</prism:number>
      <prism:startingPage>835</prism:startingPage>
      <prism:endingPage>885</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/090771430</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/090771430?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130926171?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>On Generic Nonexistence of the Schmidt--Eckart--Young Decomposition for Complex Tensors</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130926171?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/3"&gt;Volume 35, Issue 3&lt;/a&gt;, Page 886-903, January 2014. &lt;br/&gt; The Schmidt--Eckart--Young theorem for matrices states that the optimal rank-$r$ approximation of a matrix is obtained by retaining the first $r$ terms from the singular value decomposition of that matrix. This paper considers a generalization of this optimal truncation property to the rank decomposition (Candecomp/Parafac) of tensors and establishes a necessary orthogonality condition. We prove that this condition is not satisfied at least by an open set of positive Lebesgue measure in complex tensor spaces. It is proved, moreover, that for complex tensors of small rank this condition can be satisfied only by a set of tensors of Lebesgue measure zero. Finally, we demonstrate that generic tensors in cubic tensor spaces are not optimally truncatable.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 3, Page 886-903, January 2014. &lt;br/&gt; The Schmidt--Eckart--Young theorem for matrices states that the optimal rank-$r$ approximation of a matrix is obtained by retaining the first $r$ terms from the singular value decomposition of that matrix. This paper considers a generalization of this optimal truncation property to the rank decomposition (Candecomp/Parafac) of tensors and establishes a necessary orthogonality condition. We prove that this condition is not satisfied at least by an open set of positive Lebesgue measure in complex tensor spaces. It is proved, moreover, that for complex tensors of small rank this condition can be satisfied only by a set of tensors of Lebesgue measure zero. Finally, we demonstrate that generic tensors in cubic tensor spaces are not optimally truncatable.</rss:description>
      <dc:title>On Generic Nonexistence of the Schmidt--Eckart--Young Decomposition for Complex Tensors</dc:title>
      <dc:identifier>doi:10.1137/130926171</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Tue, 08 Jul 2014 07:00:00 GMT</dc:date>N. Vannieuwenhoven, J. Nicaise, R. Vandebril, and K. Meerbergen<prism:publicationName>On Generic Nonexistence of the Schmidt--Eckart--Young Decomposition for Complex Tensors</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>3</prism:number>
      <prism:startingPage>886</prism:startingPage>
      <prism:endingPage>903</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130926171</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130926171?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/13093858X?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>A New Perturbation Bound for the LDU Factorization of Diagonally Dominant Matrices</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/13093858X?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/3"&gt;Volume 35, Issue 3&lt;/a&gt;, Page 904-930, January 2014. &lt;br/&gt; This work introduces a new perturbation bound for the $L$ factor of the LDU factorization of (row) diagonally dominant matrices computed via the column diagonal dominance pivoting strategy. This strategy yields $L$ and $U$ factors which are always well-conditioned and, so, the LDU factorization is guaranteed to be a rank-revealing decomposition. The new bound together with those for the $D$ and $U$ factors in [F. M. Dopico and P. Koev, Numer. Math., 119 (2011), pp. 337--371] establish that if diagonally dominant matrices are parameterized via their diagonally dominant parts and off-diagonal entries, then tiny relative componentwise perturbations of these parameters produce tiny relative normwise variations of $L$ and $U$ and tiny relative entrywise variations of $D$ when column diagonal dominance pivoting is used. These results will allow us to prove in a follow-up work that such perturbations also lead to strong perturbation bounds for many other problems involving diagonally dominant matrices.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 3, Page 904-930, January 2014. &lt;br/&gt; This work introduces a new perturbation bound for the $L$ factor of the LDU factorization of (row) diagonally dominant matrices computed via the column diagonal dominance pivoting strategy. This strategy yields $L$ and $U$ factors which are always well-conditioned and, so, the LDU factorization is guaranteed to be a rank-revealing decomposition. The new bound together with those for the $D$ and $U$ factors in [F. M. Dopico and P. Koev, Numer. Math., 119 (2011), pp. 337--371] establish that if diagonally dominant matrices are parameterized via their diagonally dominant parts and off-diagonal entries, then tiny relative componentwise perturbations of these parameters produce tiny relative normwise variations of $L$ and $U$ and tiny relative entrywise variations of $D$ when column diagonal dominance pivoting is used. These results will allow us to prove in a follow-up work that such perturbations also lead to strong perturbation bounds for many other problems involving diagonally dominant matrices.</rss:description>
      <dc:title>A New Perturbation Bound for the LDU Factorization of Diagonally Dominant Matrices</dc:title>
      <dc:identifier>doi:10.1137/13093858X</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Tue, 08 Jul 2014 07:00:00 GMT</dc:date>Megan Dailey, Froilán M. Dopico, and Qiang Ye<prism:publicationName>A New Perturbation Bound for the LDU Factorization of Diagonally Dominant Matrices</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>3</prism:number>
      <prism:startingPage>904</prism:startingPage>
      <prism:endingPage>930</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/13093858X</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/13093858X?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/110825753?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>A Symmetry Preserving Algorithm for Matrix Scaling</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/110825753?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/3"&gt;Volume 35, Issue 3&lt;/a&gt;, Page 931-955, January 2014. &lt;br/&gt; We present an iterative algorithm which asymptotically scales the $\infty$-norm of each row and each column of a matrix to one. This scaling algorithm preserves symmetry of the original matrix and shows fast linear convergence with an asymptotic rate of 1/2. We discuss extensions of the algorithm to the 1-norm, and by inference to other norms. For the 1-norm case, we show again that convergence is linear, with the rate dependent on the spectrum of the scaled matrix.  We demonstrate experimentally that the scaling algorithm improves the conditioning of the matrix and that it helps direct solvers by reducing the need for pivoting. In particular, for symmetric matrices the theoretical and experimental results highlight the potential of the proposed algorithm over existing alternatives.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 3, Page 931-955, January 2014. &lt;br/&gt; We present an iterative algorithm which asymptotically scales the $\infty$-norm of each row and each column of a matrix to one. This scaling algorithm preserves symmetry of the original matrix and shows fast linear convergence with an asymptotic rate of 1/2. We discuss extensions of the algorithm to the 1-norm, and by inference to other norms. For the 1-norm case, we show again that convergence is linear, with the rate dependent on the spectrum of the scaled matrix.  We demonstrate experimentally that the scaling algorithm improves the conditioning of the matrix and that it helps direct solvers by reducing the need for pivoting. In particular, for symmetric matrices the theoretical and experimental results highlight the potential of the proposed algorithm over existing alternatives.</rss:description>
      <dc:title>A Symmetry Preserving Algorithm for Matrix Scaling</dc:title>
      <dc:identifier>doi:10.1137/110825753</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 17 Jul 2014 07:00:00 GMT</dc:date>Philip A. Knight, Daniel Ruiz, and Bora Uçar<prism:publicationName>A Symmetry Preserving Algorithm for Matrix Scaling</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>3</prism:number>
      <prism:startingPage>931</prism:startingPage>
      <prism:endingPage>955</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/110825753</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/110825753?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/120896499?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Birkhoff--von Neumann Theorem for Multistochastic Tensors</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/120896499?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/3"&gt;Volume 35, Issue 3&lt;/a&gt;, Page 956-973, January 2014. &lt;br/&gt; In this paper, we study the Birkhoff--von Neumann theorem for a class of multistochastic tensors. In particular, we give a necessary and sufficient condition such that a multistochastic tensor is a convex combination of finitely many permutation tensors. It is well-known that extreme points in the set of doubly stochastic matrices are just permutation matrices. However, we find that extreme points in the set of multistochastic tensors are not just permutation tensors. We provide the other types of tensors contained in the set of extreme points.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 3, Page 956-973, January 2014. &lt;br/&gt; In this paper, we study the Birkhoff--von Neumann theorem for a class of multistochastic tensors. In particular, we give a necessary and sufficient condition such that a multistochastic tensor is a convex combination of finitely many permutation tensors. It is well-known that extreme points in the set of doubly stochastic matrices are just permutation matrices. However, we find that extreme points in the set of multistochastic tensors are not just permutation tensors. We provide the other types of tensors contained in the set of extreme points.</rss:description>
      <dc:title>Birkhoff--von Neumann Theorem for Multistochastic Tensors</dc:title>
      <dc:identifier>doi:10.1137/120896499</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 17 Jul 2014 07:00:00 GMT</dc:date>Lu-Bin Cui, Wen Li, and Michael K. Ng<prism:publicationName>Birkhoff--von Neumann Theorem for Multistochastic Tensors</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>3</prism:number>
      <prism:startingPage>956</prism:startingPage>
      <prism:endingPage>973</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/120896499</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/120896499?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130914966?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>A Fast Randomized Eigensolver with Structured LDL Factorization Update</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130914966?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/3"&gt;Volume 35, Issue 3&lt;/a&gt;, Page 974-996, January 2014. &lt;br/&gt; In this paper, we propose a structured bisection method with adaptive randomized sampling for finding selected or all of the eigenvalues of certain real symmetric matrices $A$. For $A$ with a low-rank property, we construct a hierarchically semiseparable (HSS) approximation and show how to quickly evaluate and update its inertia in the bisection method. Unlike some existing randomized HSS constructions, the methods here do not require the knowledge of the off-diagonal (numerical) ranks in advance. Moreover, for $A$ with a weak rank property or slowly decaying off-diagonal singular values, we show an idea for aggressive low-rank inertia evaluation, which means that a compact HSS approximation can preserve the inertia for certain shifts. This is analytically justified for a special case, and numerically shown for more general ones. A generalized LDL factorization of the HSS approximation is then designed for the fast evaluation of the inertia. A significant advantage over standard LDL factorizations is that the HSS LDL factorization (and thus the inertia) of $A-sI$ can be quickly updated with multiple shifts $s$ in bisection. The factorization with each new shift can reuse about 60% of the work. As an important application, the structured eigensolver can be applied to symmetric Toeplitz matrices, and the cost to find one eigenvalue is nearly linear in the order of the matrix. The numerical examples demonstrate the efficiency and the accuracy of our methods, especially the benefit of low-rank inertia evaluations. The ideas and methods can be potentially adapted to other HSS computations where shifts are involved and to more problems without a significant low-rank property.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 3, Page 974-996, January 2014. &lt;br/&gt; In this paper, we propose a structured bisection method with adaptive randomized sampling for finding selected or all of the eigenvalues of certain real symmetric matrices $A$. For $A$ with a low-rank property, we construct a hierarchically semiseparable (HSS) approximation and show how to quickly evaluate and update its inertia in the bisection method. Unlike some existing randomized HSS constructions, the methods here do not require the knowledge of the off-diagonal (numerical) ranks in advance. Moreover, for $A$ with a weak rank property or slowly decaying off-diagonal singular values, we show an idea for aggressive low-rank inertia evaluation, which means that a compact HSS approximation can preserve the inertia for certain shifts. This is analytically justified for a special case, and numerically shown for more general ones. A generalized LDL factorization of the HSS approximation is then designed for the fast evaluation of the inertia. A significant advantage over standard LDL factorizations is that the HSS LDL factorization (and thus the inertia) of $A-sI$ can be quickly updated with multiple shifts $s$ in bisection. The factorization with each new shift can reuse about 60% of the work. As an important application, the structured eigensolver can be applied to symmetric Toeplitz matrices, and the cost to find one eigenvalue is nearly linear in the order of the matrix. The numerical examples demonstrate the efficiency and the accuracy of our methods, especially the benefit of low-rank inertia evaluations. The ideas and methods can be potentially adapted to other HSS computations where shifts are involved and to more problems without a significant low-rank property.</rss:description>
      <dc:title>A Fast Randomized Eigensolver with Structured LDL Factorization Update</dc:title>
      <dc:identifier>doi:10.1137/130914966</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 17 Jul 2014 07:00:00 GMT</dc:date>Yuanzhe Xi, Jianlin Xia, and Raymond Chan<prism:publicationName>A Fast Randomized Eigensolver with Structured LDL Factorization Update</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>3</prism:number>
      <prism:startingPage>974</prism:startingPage>
      <prism:endingPage>996</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130914966</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130914966?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130943455?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>An Algebraic Analysis of the Graph Modularity</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130943455?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/3"&gt;Volume 35, Issue 3&lt;/a&gt;, Page 997-1018, January 2014. &lt;br/&gt; One of the most relevant tasks in network analysis is the detection of community structures, or clustering. Most popular techniques for community detection are based on the maximization of a quality function called modularity, which in turn is based upon particular quadratic forms associated to a real symmetric modularity matrix $M$, defined in terms of the adjacency matrix and a rank-one null model matrix. That matrix could be posed inside the set of relevant matrices involved in graph theory, alongside adjacency and Laplacian matrices. In this paper we analyze certain spectral properties of modularity matrices, which are related to the community detection problem. In particular, we propose a nodal domain theorem for the eigenvectors of $M$; we point out several relations occurring between the graph's communities and nonnegative eigenvalues of $M$; and we derive a Cheeger-type inequality for the graph modularity.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 3, Page 997-1018, January 2014. &lt;br/&gt; One of the most relevant tasks in network analysis is the detection of community structures, or clustering. Most popular techniques for community detection are based on the maximization of a quality function called modularity, which in turn is based upon particular quadratic forms associated to a real symmetric modularity matrix $M$, defined in terms of the adjacency matrix and a rank-one null model matrix. That matrix could be posed inside the set of relevant matrices involved in graph theory, alongside adjacency and Laplacian matrices. In this paper we analyze certain spectral properties of modularity matrices, which are related to the community detection problem. In particular, we propose a nodal domain theorem for the eigenvectors of $M$; we point out several relations occurring between the graph's communities and nonnegative eigenvalues of $M$; and we derive a Cheeger-type inequality for the graph modularity.</rss:description>
      <dc:title>An Algebraic Analysis of the Graph Modularity</dc:title>
      <dc:identifier>doi:10.1137/130943455</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 17 Jul 2014 07:00:00 GMT</dc:date>Dario Fasino and Francesco Tudisco<prism:publicationName>An Algebraic Analysis of the Graph Modularity</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>3</prism:number>
      <prism:startingPage>997</prism:startingPage>
      <prism:endingPage>1018</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130943455</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130943455?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130945259?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Higher Order Fréchet Derivatives of Matrix Functions and the Level-2 Condition Number</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130945259?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/3"&gt;Volume 35, Issue 3&lt;/a&gt;, Page 1019-1037, January 2014. &lt;br/&gt; The Fréchet derivative $L_f$ of a matrix function                     $f \, {:} \ \mathbb{C}^{n\times n} \mapsto \mathbb{C}^{n\times n}$                     controls the sensitivity of the function to small perturbations                     in the matrix.                     While much is known about the properties of $L_f$                     and how to compute it,                     little attention has been given to higher order Fréchet derivatives.                     We derive sufficient conditions for the                     $k$th Fréchet derivative to exist and be continuous                     in its arguments                     and we develop algorithms for computing the $k$th derivative and its                     Kronecker form.                     We analyze the level-2 absolute condition number of a                     matrix function (``the condition number of the condition number'')                     and bound it in terms of the second Fréchet derivative.                     For normal matrices and the exponential we show that in the 2-norm                     the level-1 and level-2 absolute condition numbers are equal                     and that the relative condition numbers                     are within a small constant factor of each other.                     We also obtain an exact relationship between the level-1 and level-2                     absolute condition numbers for                     the matrix inverse and arbitrary nonsingular matrices,                     as well as a weaker connection for Hermitian matrices                     for a class of functions that includes the logarithm and square root.                     Finally, the relation between the level-1 and level-2 condition numbers                     is investigated more generally through numerical experiments. </content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 3, Page 1019-1037, January 2014. &lt;br/&gt; The Fréchet derivative $L_f$ of a matrix function                     $f \, {:} \ \mathbb{C}^{n\times n} \mapsto \mathbb{C}^{n\times n}$                     controls the sensitivity of the function to small perturbations                     in the matrix.                     While much is known about the properties of $L_f$                     and how to compute it,                     little attention has been given to higher order Fréchet derivatives.                     We derive sufficient conditions for the                     $k$th Fréchet derivative to exist and be continuous                     in its arguments                     and we develop algorithms for computing the $k$th derivative and its                     Kronecker form.                     We analyze the level-2 absolute condition number of a                     matrix function (``the condition number of the condition number'')                     and bound it in terms of the second Fréchet derivative.                     For normal matrices and the exponential we show that in the 2-norm                     the level-1 and level-2 absolute condition numbers are equal                     and that the relative condition numbers                     are within a small constant factor of each other.                     We also obtain an exact relationship between the level-1 and level-2                     absolute condition numbers for                     the matrix inverse and arbitrary nonsingular matrices,                     as well as a weaker connection for Hermitian matrices                     for a class of functions that includes the logarithm and square root.                     Finally, the relation between the level-1 and level-2 condition numbers                     is investigated more generally through numerical experiments. </rss:description>
      <dc:title>Higher Order Fréchet Derivatives of Matrix Functions and the Level-2 Condition Number</dc:title>
      <dc:identifier>doi:10.1137/130945259</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 24 Jul 2014 07:00:00 GMT</dc:date> Nicholas J. Higham  and  Samuel D. Relton <prism:publicationName>Higher Order Fréchet Derivatives of Matrix Functions and the Level-2 Condition Number</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>3</prism:number>
      <prism:startingPage>1019</prism:startingPage>
      <prism:endingPage>1037</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130945259</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130945259?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130945995?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>An Accelerated Divide-and-Conquer Algorithm for the Bidiagonal SVD Problem</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130945995?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/3"&gt;Volume 35, Issue 3&lt;/a&gt;, Page 1038-1057, January 2014. &lt;br/&gt; In this paper, aiming at solving the bidiagonal SVD problem, a classical divide-and-conquer (DC) algorithm is modified, which needs to compute the SVD of broken arrow matrices by solving secular equations. The main cost of DC lies in the updating of singular vectors, which involves two matrix-matrix multiplications. We find that the singular vector matrices of a broken arrow matrix are Cauchy-like matrices and have an off-diagonal low-rank property, so they can be approximated efficiently by hierarchically semiseparable (HSS) matrices. Hereby, by using the HSS techniques, the complexity of computing singular vectors can be reduced significantly. An accelerated DC algorithm is proposed, denoted by ADC. Furthermore, we use a structured low-rank approximation method to construct these HSS approximations. Numerous experiments show ADC is both fast and numerically stable. When dealing with large matrices with few deflations, ADC can be 3x faster than DC in the optimized LAPACK libraries such as Intel MKL without any degradation in accuracy. These techniques can be used to similarly solve the symmetric tridiagonal eigenvalue problem.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 3, Page 1038-1057, January 2014. &lt;br/&gt; In this paper, aiming at solving the bidiagonal SVD problem, a classical divide-and-conquer (DC) algorithm is modified, which needs to compute the SVD of broken arrow matrices by solving secular equations. The main cost of DC lies in the updating of singular vectors, which involves two matrix-matrix multiplications. We find that the singular vector matrices of a broken arrow matrix are Cauchy-like matrices and have an off-diagonal low-rank property, so they can be approximated efficiently by hierarchically semiseparable (HSS) matrices. Hereby, by using the HSS techniques, the complexity of computing singular vectors can be reduced significantly. An accelerated DC algorithm is proposed, denoted by ADC. Furthermore, we use a structured low-rank approximation method to construct these HSS approximations. Numerous experiments show ADC is both fast and numerically stable. When dealing with large matrices with few deflations, ADC can be 3x faster than DC in the optimized LAPACK libraries such as Intel MKL without any degradation in accuracy. These techniques can be used to similarly solve the symmetric tridiagonal eigenvalue problem.</rss:description>
      <dc:title>An Accelerated Divide-and-Conquer Algorithm for the Bidiagonal SVD Problem</dc:title>
      <dc:identifier>doi:10.1137/130945995</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 07 Aug 2014 07:00:00 GMT</dc:date>Shengguo Li, Ming Gu, Lizhi Cheng, Xuebin Chi, and Meng Sun<prism:publicationName>An Accelerated Divide-and-Conquer Algorithm for the Bidiagonal SVD Problem</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>3</prism:number>
      <prism:startingPage>1038</prism:startingPage>
      <prism:endingPage>1057</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130945995</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130945995?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130938207?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>On the Global Convergence of the Alternating Least Squares Method for Rank-One Approximation to Generic Tensors</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130938207?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/3"&gt;Volume 35, Issue 3&lt;/a&gt;, Page 1058-1072, January 2014. &lt;br/&gt; Tensor decomposition has important applications in various disciplines, but it remains  an extremely challenging task even to this date. A slightly more manageable endeavor  has been to find a low rank approximation in place of the decomposition. Even for this less stringent undertaking, it is an established fact that tensors beyond matrices can fail to have best low rank approximations, with the notable exception that the best rank-one approximation always exists for tensors of any order. Toward the latter, the most popular approach is the notion of alternating least squares whose specific numerical scheme appears in the form as a variant of the power method. Though the limiting behavior of the objective values is well understood, a proof of global convergence for the iterates themselves has been elusive. This paper partially addresses the missing piece by showing that for almost all tensors, the iterates generated by the alternating least squares method for the rank-one approximation converge globally. The underlying technique employed is an eclectic mix of knowledge from algebraic geometry and dynamical system.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 3, Page 1058-1072, January 2014. &lt;br/&gt; Tensor decomposition has important applications in various disciplines, but it remains  an extremely challenging task even to this date. A slightly more manageable endeavor  has been to find a low rank approximation in place of the decomposition. Even for this less stringent undertaking, it is an established fact that tensors beyond matrices can fail to have best low rank approximations, with the notable exception that the best rank-one approximation always exists for tensors of any order. Toward the latter, the most popular approach is the notion of alternating least squares whose specific numerical scheme appears in the form as a variant of the power method. Though the limiting behavior of the objective values is well understood, a proof of global convergence for the iterates themselves has been elusive. This paper partially addresses the missing piece by showing that for almost all tensors, the iterates generated by the alternating least squares method for the rank-one approximation converge globally. The underlying technique employed is an eclectic mix of knowledge from algebraic geometry and dynamical system.</rss:description>
      <dc:title>On the Global Convergence of the Alternating Least Squares Method for Rank-One Approximation to Generic Tensors</dc:title>
      <dc:identifier>doi:10.1137/130938207</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 07 Aug 2014 07:00:00 GMT</dc:date>Liqi Wang and Moody T. Chu<prism:publicationName>On the Global Convergence of the Alternating Least Squares Method for Rank-One Approximation to Generic Tensors</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>3</prism:number>
      <prism:startingPage>1058</prism:startingPage>
      <prism:endingPage>1072</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130938207</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130938207?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/120869286?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>The Exact Condition Number of the Truncated Singular Value Solution of a Linear Ill-Posed Problem</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/120869286?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/3"&gt;Volume 35, Issue 3&lt;/a&gt;, Page 1073-1085, January 2014. &lt;br/&gt; The main result of this paper is the formulation of an explicit expression for the condition number of the truncated least squares solution of $Ax=b$. This expression is given in terms of the singular values of $A$ and the Fourier coefficients of $b$. The result is derived using the notion of the Fréchet derivative together with the product norm on  the data $[A,b]$ and the 2-norm on the solution. Numerical experiments are given to confirm our results by comparing them to those obtained by means of a finite difference approach.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 3, Page 1073-1085, January 2014. &lt;br/&gt; The main result of this paper is the formulation of an explicit expression for the condition number of the truncated least squares solution of $Ax=b$. This expression is given in terms of the singular values of $A$ and the Fourier coefficients of $b$. The result is derived using the notion of the Fréchet derivative together with the product norm on  the data $[A,b]$ and the 2-norm on the solution. Numerical experiments are given to confirm our results by comparing them to those obtained by means of a finite difference approach.</rss:description>
      <dc:title>The Exact Condition Number of the Truncated Singular Value Solution of a Linear Ill-Posed Problem</dc:title>
      <dc:identifier>doi:10.1137/120869286</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 14 Aug 2014 07:00:00 GMT</dc:date>El Houcine Bergou, Serge Gratton, and Jean Tshimanga<prism:publicationName>The Exact Condition Number of the Truncated Singular Value Solution of a Linear Ill-Posed Problem</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>3</prism:number>
      <prism:startingPage>1073</prism:startingPage>
      <prism:endingPage>1085</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/120869286</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/120869286?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130917260?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Best Kronecker Product Approximation of The Blurring Operator in Three Dimensional Image Restoration Problems</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130917260?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/3"&gt;Volume 35, Issue 3&lt;/a&gt;, Page 1086-1104, January 2014. &lt;br/&gt; In this paper, we propose a method to find the best Kronecker product approximation of the blurring operator which arises in three dimensional image restoration problems. We show that this problem can be reduced to a well known rank-1 approximation of the scaled three dimensional point spread function (PSF) array, which is much smaller. This approximation can be used as a preconditioner in solving image restoration problems with iterative methods.  The comparison of the approximation by the new scaled PSF array and approximation by the original PSF array that is used in [J. G. Nagy and M. E. Kilmer, IEEE Trans. Image Process., 15 (2006), pp. 604--613], confirms the performance of the new proposed approximation.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 3, Page 1086-1104, January 2014. &lt;br/&gt; In this paper, we propose a method to find the best Kronecker product approximation of the blurring operator which arises in three dimensional image restoration problems. We show that this problem can be reduced to a well known rank-1 approximation of the scaled three dimensional point spread function (PSF) array, which is much smaller. This approximation can be used as a preconditioner in solving image restoration problems with iterative methods.  The comparison of the approximation by the new scaled PSF array and approximation by the original PSF array that is used in [J. G. Nagy and M. E. Kilmer, IEEE Trans. Image Process., 15 (2006), pp. 604--613], confirms the performance of the new proposed approximation.</rss:description>
      <dc:title>Best Kronecker Product Approximation of The Blurring Operator in Three Dimensional Image Restoration Problems</dc:title>
      <dc:identifier>doi:10.1137/130917260</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Tue, 19 Aug 2014 07:00:00 GMT</dc:date>Mansoor Rezghi, S. Mohammad Hosseini, and Lars Eldén<prism:publicationName>Best Kronecker Product Approximation of The Blurring Operator in Three Dimensional Image Restoration Problems</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>3</prism:number>
      <prism:startingPage>1086</prism:startingPage>
      <prism:endingPage>1104</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130917260</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130917260?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130940414?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Fast Updating Algorithms for Latent Semantic Indexing</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130940414?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/3"&gt;Volume 35, Issue 3&lt;/a&gt;, Page 1105-1131, January 2014. &lt;br/&gt; This paper discusses a few algorithms for updating the approximate singular value decomposition (SVD) in the context of information retrieval by latent semantic indexing (LSI) methods. A unifying framework is considered which is based on Rayleigh--Ritz projection methods. First, a Rayleigh--Ritz approach for the SVD is discussed and it is then used to interpret the Zha and Simon algorithms [SIAM J. Sci. Comput., 21 (1999), pp. 782--791]. This viewpoint leads to a few alternatives whose goal is to reduce computational cost and storage requirement by projection techniques that utilize subspaces of much smaller dimension. Numerical experiments show that the proposed algorithms yield accuracies comparable to those obtained from standard ones at a much lower  computational cost.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 3, Page 1105-1131, January 2014. &lt;br/&gt; This paper discusses a few algorithms for updating the approximate singular value decomposition (SVD) in the context of information retrieval by latent semantic indexing (LSI) methods. A unifying framework is considered which is based on Rayleigh--Ritz projection methods. First, a Rayleigh--Ritz approach for the SVD is discussed and it is then used to interpret the Zha and Simon algorithms [SIAM J. Sci. Comput., 21 (1999), pp. 782--791]. This viewpoint leads to a few alternatives whose goal is to reduce computational cost and storage requirement by projection techniques that utilize subspaces of much smaller dimension. Numerical experiments show that the proposed algorithms yield accuracies comparable to those obtained from standard ones at a much lower  computational cost.</rss:description>
      <dc:title>Fast Updating Algorithms for Latent Semantic Indexing</dc:title>
      <dc:identifier>doi:10.1137/130940414</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Tue, 19 Aug 2014 07:00:00 GMT</dc:date>Eugene Vecharynski and Yousef Saad<prism:publicationName>Fast Updating Algorithms for Latent Semantic Indexing</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>3</prism:number>
      <prism:startingPage>1105</prism:startingPage>
      <prism:endingPage>1131</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130940414</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130940414?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130909949?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>A Logarithmic Minimization Property of the Unitary Polar Factor in the Spectral and Frobenius Norms</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130909949?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/3"&gt;Volume 35, Issue 3&lt;/a&gt;, Page 1132-1154, January 2014. &lt;br/&gt; The unitary polar factor $Q=U_p$ in the polar decomposition of $Z=U_p \, H$ is the minimizer over unitary matrices $Q$ for both $\|{\rm Log}(Q^* Z)\|^2$ and its Hermitian part $\|{{\rm sym}{_{_*}}\!}({\rm Log}(Q^* Z))\|^2$ over both $\mathbb{R}$ and $\mathbb{C}$ for any given invertible matrix $Z\in\mathbb{C}^{n\times n}$ and any matrix logarithm Log, not necessarily the principal logarithm log. We prove this  for the spectral matrix norm for any $n$ and for the Frobenius matrix norm for $n\leq 3$. The result shows that the unitary polar factor is the nearest orthogonal matrix to $Z$ not only in the normwise sense but also in a geodesic distance. The derivation is based on Bhatia's generalization of Bernstein's trace inequality for the matrix exponential and a new sum of squared logarithms inequality. Our result generalizes the fact for scalars that for any complex logarithm and for all $z\in\mathbb{C}{\setminus}\{0\} \min_{\vartheta\in(-\pi,\pi]}{|{\rm Log}_{\mathbb{C}}(e^{-i\vartheta} z)|}^2={|{\rm log}|{z}||}^2$, $\min_{\vartheta\in(-\pi,\pi]}{|\,{{\mathfrak{Re}}}\,{\rm Log}_{\mathbb{C}}(e^{-i\vartheta} z)|}^2={|{\rm log}|{z}||}^2$.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 3, Page 1132-1154, January 2014. &lt;br/&gt; The unitary polar factor $Q=U_p$ in the polar decomposition of $Z=U_p \, H$ is the minimizer over unitary matrices $Q$ for both $\|{\rm Log}(Q^* Z)\|^2$ and its Hermitian part $\|{{\rm sym}{_{_*}}\!}({\rm Log}(Q^* Z))\|^2$ over both $\mathbb{R}$ and $\mathbb{C}$ for any given invertible matrix $Z\in\mathbb{C}^{n\times n}$ and any matrix logarithm Log, not necessarily the principal logarithm log. We prove this  for the spectral matrix norm for any $n$ and for the Frobenius matrix norm for $n\leq 3$. The result shows that the unitary polar factor is the nearest orthogonal matrix to $Z$ not only in the normwise sense but also in a geodesic distance. The derivation is based on Bhatia's generalization of Bernstein's trace inequality for the matrix exponential and a new sum of squared logarithms inequality. Our result generalizes the fact for scalars that for any complex logarithm and for all $z\in\mathbb{C}{\setminus}\{0\} \min_{\vartheta\in(-\pi,\pi]}{|{\rm Log}_{\mathbb{C}}(e^{-i\vartheta} z)|}^2={|{\rm log}|{z}||}^2$, $\min_{\vartheta\in(-\pi,\pi]}{|\,{{\mathfrak{Re}}}\,{\rm Log}_{\mathbb{C}}(e^{-i\vartheta} z)|}^2={|{\rm log}|{z}||}^2$.</rss:description>
      <dc:title>A Logarithmic Minimization Property of the Unitary Polar Factor in the Spectral and Frobenius Norms</dc:title>
      <dc:identifier>doi:10.1137/130909949</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 28 Aug 2014 07:00:00 GMT</dc:date>Patrizio Neff, Yuji Nakatsukasa, and Andreas Fischle<prism:publicationName>A Logarithmic Minimization Property of the Unitary Polar Factor in the Spectral and Frobenius Norms</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>3</prism:number>
      <prism:startingPage>1132</prism:startingPage>
      <prism:endingPage>1154</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130909949</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130909949?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130935112?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Semidefinite Relaxations for Best Rank-1 Tensor Approximations</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130935112?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/3"&gt;Volume 35, Issue 3&lt;/a&gt;, Page 1155-1179, January 2014. &lt;br/&gt; This paper studies the problem of finding best rank-1 approximations for both symmetric and nonsymmetric tensors. For symmetric tensors, this is equivalent to optimizing homogeneous polynomials over unit spheres; for nonsymmetric tensors, this is equivalent to optimizing multiquadratic forms over multispheres. We propose semidefinite relaxations, based on sum of squares representations, to solve these polynomial optimization problems. Their special properties and structures are studied. In applications, the resulting semidefinite programs are often large scale. The recent Newton-CG augmented Lagrangian method by Zhao, Sun, and Toh [SIAM J. Optim., 20 (2010), pp. 1737--1765] is suitable for solving these semidefinite relaxations. Extensive numerical experiments are presented to show that this approach is efficient in getting best rank-1 approximations.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 3, Page 1155-1179, January 2014. &lt;br/&gt; This paper studies the problem of finding best rank-1 approximations for both symmetric and nonsymmetric tensors. For symmetric tensors, this is equivalent to optimizing homogeneous polynomials over unit spheres; for nonsymmetric tensors, this is equivalent to optimizing multiquadratic forms over multispheres. We propose semidefinite relaxations, based on sum of squares representations, to solve these polynomial optimization problems. Their special properties and structures are studied. In applications, the resulting semidefinite programs are often large scale. The recent Newton-CG augmented Lagrangian method by Zhao, Sun, and Toh [SIAM J. Optim., 20 (2010), pp. 1737--1765] is suitable for solving these semidefinite relaxations. Extensive numerical experiments are presented to show that this approach is efficient in getting best rank-1 approximations.</rss:description>
      <dc:title>Semidefinite Relaxations for Best Rank-1 Tensor Approximations</dc:title>
      <dc:identifier>doi:10.1137/130935112</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 28 Aug 2014 07:00:00 GMT</dc:date>Jiawang Nie and Li Wang<prism:publicationName>Semidefinite Relaxations for Best Rank-1 Tensor Approximations</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>3</prism:number>
      <prism:startingPage>1155</prism:startingPage>
      <prism:endingPage>1179</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130935112</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130935112?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130931655?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Factorization Approach to Structured Low-Rank Approximation with Applications</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130931655?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/3"&gt;Volume 35, Issue 3&lt;/a&gt;, Page 1180-1204, January 2014. &lt;br/&gt; We consider the problem of approximating an affinely structured matrix, for example, a Hankel matrix, by a low-rank matrix with the same structure. This problem occurs in system identification, signal processing, and computer algebra, among others. We impose the low rank by modeling the approximation as a product of two factors with reduced dimension. The structure of the low-rank model is enforced by introducing a penalty term in the objective function. The proposed local optimization algorithm is able to solve the weighted structured low-rank approximation problem, as well as to deal with the cases of missing or fixed elements. In contrast to approaches based on kernel representations (in the linear algebraic sense), the proposed algorithm is designed to address the case of small targeted rank. We compare it to existing approaches on numerical examples of system identification, approximate greatest common divisor problem, and symmetric tensor decomposition and demonstrate its consistently good performance.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 3, Page 1180-1204, January 2014. &lt;br/&gt; We consider the problem of approximating an affinely structured matrix, for example, a Hankel matrix, by a low-rank matrix with the same structure. This problem occurs in system identification, signal processing, and computer algebra, among others. We impose the low rank by modeling the approximation as a product of two factors with reduced dimension. The structure of the low-rank model is enforced by introducing a penalty term in the objective function. The proposed local optimization algorithm is able to solve the weighted structured low-rank approximation problem, as well as to deal with the cases of missing or fixed elements. In contrast to approaches based on kernel representations (in the linear algebraic sense), the proposed algorithm is designed to address the case of small targeted rank. We compare it to existing approaches on numerical examples of system identification, approximate greatest common divisor problem, and symmetric tensor decomposition and demonstrate its consistently good performance.</rss:description>
      <dc:title>Factorization Approach to Structured Low-Rank Approximation with Applications</dc:title>
      <dc:identifier>doi:10.1137/130931655</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 11 Sep 2014 07:00:00 GMT</dc:date>Mariya Ishteva, Konstantin Usevich, and Ivan Markovsky<prism:publicationName>Factorization Approach to Structured Low-Rank Approximation with Applications</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>3</prism:number>
      <prism:startingPage>1180</prism:startingPage>
      <prism:endingPage>1204</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130931655</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130931655?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/140953150?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Fast Enclosure for All Eigenvalues and Invariant Subspaces in Generalized Eigenvalue Problems</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/140953150?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/3"&gt;Volume 35, Issue 3&lt;/a&gt;, Page 1205-1225, January 2014. &lt;br/&gt; Two fast algorithms for enclosing all eigenvalues and invariant subspaces in generalized eigenvalue problems are proposed. In these algorithms, individual eigenvectors and invariant subspaces are enclosed when eigenvalues are well separated and closely clustered, respectively. The first algorithm involves only cubic complexity and automatically determines eigenvalue clusters. The second algorithm is applicable even for defective eigenvalues. Numerical results show the properties of the proposed algorithms.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 3, Page 1205-1225, January 2014. &lt;br/&gt; Two fast algorithms for enclosing all eigenvalues and invariant subspaces in generalized eigenvalue problems are proposed. In these algorithms, individual eigenvectors and invariant subspaces are enclosed when eigenvalues are well separated and closely clustered, respectively. The first algorithm involves only cubic complexity and automatically determines eigenvalue clusters. The second algorithm is applicable even for defective eigenvalues. Numerical results show the properties of the proposed algorithms.</rss:description>
      <dc:title>Fast Enclosure for All Eigenvalues and Invariant Subspaces in Generalized Eigenvalue Problems</dc:title>
      <dc:identifier>doi:10.1137/140953150</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 11 Sep 2014 07:00:00 GMT</dc:date>Shinya Miyajima<prism:publicationName>Fast Enclosure for All Eigenvalues and Invariant Subspaces in Generalized Eigenvalue Problems</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>3</prism:number>
      <prism:startingPage>1205</prism:startingPage>
      <prism:endingPage>1225</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/140953150</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/140953150?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/120894294?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Aggressively Truncated Taylor Series Method for Accurate Computation of Exponentials of Essentially Nonnegative Matrices</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/120894294?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/2"&gt;Volume 35, Issue 2&lt;/a&gt;, Page 317-338, January 2014. &lt;br/&gt; Small relative perturbations to the entries of an essentially nonnegative matrix introduce small relative errors to entries of its exponential. It is thus desirable to compute the exponential with high componentwise relative accuracy. Taylor series approximation coupled with scaling and squaring is used to compute the exponential of an essentially nonnegative matrix. An a priori componentwise relative error bound of truncation is established, from which one can choose the degree of Taylor series expansion and the scale factor so that the exponential is computed with desired componentwise relative accuracy. To reduce the computational cost, the degree of the Taylor series expansion is chosen small, while the scale factor is chosen sufficiently large to achieve the desired accuracy. The rounding errors in the squaring stage are not serious as squaring is forward stable for nonnegative matrices. We also establish a posteriori componentwise error bounds and derive a novel interval algorithm for the matrix exponential of an essentially nonnegative matrix. Rounding error analysis and numerical experiments demonstrate the efficiency and accuracy of the proposed methods.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 2, Page 317-338, January 2014. &lt;br/&gt; Small relative perturbations to the entries of an essentially nonnegative matrix introduce small relative errors to entries of its exponential. It is thus desirable to compute the exponential with high componentwise relative accuracy. Taylor series approximation coupled with scaling and squaring is used to compute the exponential of an essentially nonnegative matrix. An a priori componentwise relative error bound of truncation is established, from which one can choose the degree of Taylor series expansion and the scale factor so that the exponential is computed with desired componentwise relative accuracy. To reduce the computational cost, the degree of the Taylor series expansion is chosen small, while the scale factor is chosen sufficiently large to achieve the desired accuracy. The rounding errors in the squaring stage are not serious as squaring is forward stable for nonnegative matrices. We also establish a posteriori componentwise error bounds and derive a novel interval algorithm for the matrix exponential of an essentially nonnegative matrix. Rounding error analysis and numerical experiments demonstrate the efficiency and accuracy of the proposed methods.</rss:description>
      <dc:title>Aggressively Truncated Taylor Series Method for Accurate Computation of Exponentials of Essentially Nonnegative Matrices</dc:title>
      <dc:identifier>doi:10.1137/120894294</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Tue, 01 Apr 2014 07:00:00 GMT</dc:date>Meiyue Shao, Weiguo Gao, and Jungong Xue<prism:publicationName>Aggressively Truncated Taylor Series Method for Accurate Computation of Exponentials of Essentially Nonnegative Matrices</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>2</prism:number>
      <prism:startingPage>317</prism:startingPage>
      <prism:endingPage>338</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/120894294</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/120894294?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130934933?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>The Antitriangular Factorization of Saddle Point Matrices</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130934933?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/2"&gt;Volume 35, Issue 2&lt;/a&gt;, Page 339-353, January 2014. &lt;br/&gt; Mastronardi and Van Dooren [SIAM J. Matrix Anal. Appl., 34 (2013), pp. 173--196] recently introduced the block antitriangular (``Batman'') decomposition for symmetric indefinite matrices. Here we show the simplification of this factorization for saddle point matrices and demonstrate how it represents the common nullspace method. We show that rank-1 updates to the saddle point matrix can be easily incorporated into the factorization and give bounds on the eigenvalues of matrices important in saddle point theory. We show the relation of this factorization to constraint preconditioning and how it transforms but preserves the structure of block diagonal and block triangular preconditioners.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 2, Page 339-353, January 2014. &lt;br/&gt; Mastronardi and Van Dooren [SIAM J. Matrix Anal. Appl., 34 (2013), pp. 173--196] recently introduced the block antitriangular (``Batman'') decomposition for symmetric indefinite matrices. Here we show the simplification of this factorization for saddle point matrices and demonstrate how it represents the common nullspace method. We show that rank-1 updates to the saddle point matrix can be easily incorporated into the factorization and give bounds on the eigenvalues of matrices important in saddle point theory. We show the relation of this factorization to constraint preconditioning and how it transforms but preserves the structure of block diagonal and block triangular preconditioners.</rss:description>
      <dc:title>The Antitriangular Factorization of Saddle Point Matrices</dc:title>
      <dc:identifier>doi:10.1137/130934933</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Tue, 01 Apr 2014 07:00:00 GMT</dc:date>J. Pestana and A. J. Wathen<prism:publicationName>The Antitriangular Factorization of Saddle Point Matrices</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>2</prism:number>
      <prism:startingPage>339</prism:startingPage>
      <prism:endingPage>353</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130934933</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130934933?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/13090866X?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>FEAST As A Subspace Iteration Eigensolver Accelerated By Approximate Spectral Projection</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/13090866X?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/2"&gt;Volume 35, Issue 2&lt;/a&gt;, Page 354-390, January 2014. &lt;br/&gt; The calculation of a segment of eigenvalues and their corresponding eigenvectors of a Hermitian matrix or matrix pencil has many applications. A new density-matrix-based algorithm has been proposed recently and a software package FEAST has been developed. The density-matrix approach allows FEAST's implementation to exploit a key strength of modern computer architectures, namely, multiple levels of parallelism. Consequently, the software package has been well received, especially in the electronic structure community. Nevertheless, theoretical analysis of FEAST has lagged. For instance, the FEAST algorithm has not been proven to converge. This paper offers a detailed numerical analysis of FEAST. In particular, we show that the FEAST algorithm can be understood as an accelerated subspace iteration algorithm in conjunction with the Rayleigh--Ritz procedure. The novelty of FEAST lies in its accelerator, which is a rational matrix function that approximates the spectral projector onto the eigenspace in question. Analysis of the numerical nature of this approximate spectral projector and the resulting subspaces generated in the FEAST algorithm establishes the algorithm's convergence. This paper shows that FEAST is resilient against rounding errors and establishes properties that can be leveraged to enhance the algorithm's robustness. Finally, we propose an extension of FEAST to handle non-Hermitian problems and suggest some future research directions.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 2, Page 354-390, January 2014. &lt;br/&gt; The calculation of a segment of eigenvalues and their corresponding eigenvectors of a Hermitian matrix or matrix pencil has many applications. A new density-matrix-based algorithm has been proposed recently and a software package FEAST has been developed. The density-matrix approach allows FEAST's implementation to exploit a key strength of modern computer architectures, namely, multiple levels of parallelism. Consequently, the software package has been well received, especially in the electronic structure community. Nevertheless, theoretical analysis of FEAST has lagged. For instance, the FEAST algorithm has not been proven to converge. This paper offers a detailed numerical analysis of FEAST. In particular, we show that the FEAST algorithm can be understood as an accelerated subspace iteration algorithm in conjunction with the Rayleigh--Ritz procedure. The novelty of FEAST lies in its accelerator, which is a rational matrix function that approximates the spectral projector onto the eigenspace in question. Analysis of the numerical nature of this approximate spectral projector and the resulting subspaces generated in the FEAST algorithm establishes the algorithm's convergence. This paper shows that FEAST is resilient against rounding errors and establishes properties that can be leveraged to enhance the algorithm's robustness. Finally, we propose an extension of FEAST to handle non-Hermitian problems and suggest some future research directions.</rss:description>
      <dc:title>FEAST As A Subspace Iteration Eigensolver Accelerated By Approximate Spectral Projection</dc:title>
      <dc:identifier>doi:10.1137/13090866X</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 03 Apr 2014 07:00:00 GMT</dc:date>Ping Tak Peter Tang and Eric Polizzi<prism:publicationName>FEAST As A Subspace Iteration Eigensolver Accelerated By Approximate Spectral Projection</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>2</prism:number>
      <prism:startingPage>354</prism:startingPage>
      <prism:endingPage>390</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/13090866X</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/13090866X?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130907811?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Lifted Polytope Methods for Computing the Joint Spectral Radius</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130907811?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/2"&gt;Volume 35, Issue 2&lt;/a&gt;, Page 391-410, January 2014. &lt;br/&gt; We present new methods for computing the joint spectral radius of finite sets of matrices. The methods build on two ideas that previously appeared in the literature: the polytope norm iterative construction, and the lifting procedure.  Moreover, the combination of these two ideas allows us to introduce a pruning algorithm which can importantly reduce the computational burden. We prove several theoretical properties of our methods, such as finiteness computational result which extends a known result for unlifted sets of matrices, and provide numerical examples of their good behavior.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 2, Page 391-410, January 2014. &lt;br/&gt; We present new methods for computing the joint spectral radius of finite sets of matrices. The methods build on two ideas that previously appeared in the literature: the polytope norm iterative construction, and the lifting procedure.  Moreover, the combination of these two ideas allows us to introduce a pruning algorithm which can importantly reduce the computational burden. We prove several theoretical properties of our methods, such as finiteness computational result which extends a known result for unlifted sets of matrices, and provide numerical examples of their good behavior.</rss:description>
      <dc:title>Lifted Polytope Methods for Computing the Joint Spectral Radius</dc:title>
      <dc:identifier>doi:10.1137/130907811</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 03 Apr 2014 07:00:00 GMT</dc:date>Raphaël M. Jungers, Antonio Cicone, and Nicola Guglielmi<prism:publicationName>Lifted Polytope Methods for Computing the Joint Spectral Radius</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>2</prism:number>
      <prism:startingPage>391</prism:startingPage>
      <prism:endingPage>410</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130907811</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130907811?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/110858148?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Computing a Partial Schur Factorization of Nonlinear Eigenvalue Problems Using the Infinite Arnoldi Method</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/110858148?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/2"&gt;Volume 35, Issue 2&lt;/a&gt;, Page 411-436, January 2014. &lt;br/&gt; The partial Schur factorization can be used to  represent several eigenpairs of a matrix in a numerically robust way. Different adaptions of the Arnoldi method are often used to compute partial Schur factorizations. We propose here a technique to compute a partial Schur factorization of a nonlinear eigenvalue problem (NEP). The technique is an extension of our algorithm from [E. Jarlebring, W. Michiels, and K. Meerbergen, Numer. Math., 122 (2012), pp. 169--195], now called the infinite Arnoldi method. The infinite Arnoldi method is a method designed for NEPs, and can be interpreted as Arnoldi's method applied to a linear infinite-dimensional operator, whose reciprocal eigenvalues are the solutions to the NEP. As a first result we show that the invariant pairs of the operator are equivalent to invariant pairs of the NEP. We characterize the structure of the invariant pairs of the operator and show how one can carry out a modification of the infinite Arnoldi method by respecting the structure. This also allows us to naturally add the feature known as locking. We  nest this algorithm with an outer iteration, where the infinite Arnoldi method for a particular type of structured functions is appropriately restarted. The restarting exploits the structure and is inspired by the well-known implicitly restarted Arnoldi method for standard eigenvalue problems. The final algorithm is applied to examples from a benchmark collection, showing that both processing time and memory consumption can be considerably reduced with the restarting technique.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 2, Page 411-436, January 2014. &lt;br/&gt; The partial Schur factorization can be used to  represent several eigenpairs of a matrix in a numerically robust way. Different adaptions of the Arnoldi method are often used to compute partial Schur factorizations. We propose here a technique to compute a partial Schur factorization of a nonlinear eigenvalue problem (NEP). The technique is an extension of our algorithm from [E. Jarlebring, W. Michiels, and K. Meerbergen, Numer. Math., 122 (2012), pp. 169--195], now called the infinite Arnoldi method. The infinite Arnoldi method is a method designed for NEPs, and can be interpreted as Arnoldi's method applied to a linear infinite-dimensional operator, whose reciprocal eigenvalues are the solutions to the NEP. As a first result we show that the invariant pairs of the operator are equivalent to invariant pairs of the NEP. We characterize the structure of the invariant pairs of the operator and show how one can carry out a modification of the infinite Arnoldi method by respecting the structure. This also allows us to naturally add the feature known as locking. We  nest this algorithm with an outer iteration, where the infinite Arnoldi method for a particular type of structured functions is appropriately restarted. The restarting exploits the structure and is inspired by the well-known implicitly restarted Arnoldi method for standard eigenvalue problems. The final algorithm is applied to examples from a benchmark collection, showing that both processing time and memory consumption can be considerably reduced with the restarting technique.</rss:description>
      <dc:title>Computing a Partial Schur Factorization of Nonlinear Eigenvalue Problems Using the Infinite Arnoldi Method</dc:title>
      <dc:identifier>doi:10.1137/110858148</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Tue, 08 Apr 2014 07:00:00 GMT</dc:date>Elias Jarlebring, Karl Meerbergen, and Wim Michiels<prism:publicationName>Computing a Partial Schur Factorization of Nonlinear Eigenvalue Problems Using the Infinite Arnoldi Method</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>2</prism:number>
      <prism:startingPage>411</prism:startingPage>
      <prism:endingPage>436</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/110858148</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/110858148?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130915339?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>$M$-Tensors and Some Applications</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130915339?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/2"&gt;Volume 35, Issue 2&lt;/a&gt;, Page 437-452, January 2014. &lt;br/&gt; We introduce $M$-tensors. This concept extends the concept of $M$-matrices. We denote $Z$-tensors as the tensors with nonpositive off-diagonal entries. We show that $M$-tensors must be $Z$-tensors and the maximal diagonal entry must be nonnegative. The diagonal elements of a symmetric $M$-tensor must be nonnegative. A symmetric $M$-tensor is copositive. Based on the spectral theory of nonnegative tensors, we show that the minimal value of the real parts of all eigenvalues of an $M$-tensor is its smallest H$^+$-eigenvalue and also is its smallest H-eigenvalue. We show that a $Z$-tensor is an $M$-tensor if and only if all its H$^+$-eigenvalues are nonnegative. Some further spectral properties of $M$-tensors are given.  We also introduce strong $M$-tensors, and some corresponding conclusions are given. In particular, we show that all $H$-eigenvalues of strong $M$-tensors are positive. We apply this property to study the positive definiteness of a class of multivariate forms associated with $Z$-tensors. We also propose an algorithm for testing the positive definiteness of such a multivariate form.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 2, Page 437-452, January 2014. &lt;br/&gt; We introduce $M$-tensors. This concept extends the concept of $M$-matrices. We denote $Z$-tensors as the tensors with nonpositive off-diagonal entries. We show that $M$-tensors must be $Z$-tensors and the maximal diagonal entry must be nonnegative. The diagonal elements of a symmetric $M$-tensor must be nonnegative. A symmetric $M$-tensor is copositive. Based on the spectral theory of nonnegative tensors, we show that the minimal value of the real parts of all eigenvalues of an $M$-tensor is its smallest H$^+$-eigenvalue and also is its smallest H-eigenvalue. We show that a $Z$-tensor is an $M$-tensor if and only if all its H$^+$-eigenvalues are nonnegative. Some further spectral properties of $M$-tensors are given.  We also introduce strong $M$-tensors, and some corresponding conclusions are given. In particular, we show that all $H$-eigenvalues of strong $M$-tensors are positive. We apply this property to study the positive definiteness of a class of multivariate forms associated with $Z$-tensors. We also propose an algorithm for testing the positive definiteness of such a multivariate form.</rss:description>
      <dc:title>$M$-Tensors and Some Applications</dc:title>
      <dc:identifier>doi:10.1137/130915339</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Tue, 08 Apr 2014 07:00:00 GMT</dc:date>Liping Zhang, Liqun Qi, and Guanglu Zhou<prism:publicationName>$M$-Tensors and Some Applications</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>2</prism:number>
      <prism:startingPage>437</prism:startingPage>
      <prism:endingPage>452</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130915339</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130915339?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130925621?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Structured Eigenvalue Backward Errors of Matrix Pencils and Polynomials with Hermitian and Related Structures</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130925621?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/2"&gt;Volume 35, Issue 2&lt;/a&gt;, Page 453-475, January 2014. &lt;br/&gt; We derive a formula for the backward error of a complex number $\lambda$ when considered as an approximate eigenvalue of a Hermitian matrix pencil or polynomial with respect to Hermitian perturbations. The same are also obtained for approximate eigenvalues of matrix pencils and polynomials with related structures like skew-Hermitian, $*$-even, and $*$-odd. Numerical experiments suggest that in many cases there is a significant difference between the backward errors with respect to  perturbations that preserve structure and those with respect to arbitrary perturbations.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 2, Page 453-475, January 2014. &lt;br/&gt; We derive a formula for the backward error of a complex number $\lambda$ when considered as an approximate eigenvalue of a Hermitian matrix pencil or polynomial with respect to Hermitian perturbations. The same are also obtained for approximate eigenvalues of matrix pencils and polynomials with related structures like skew-Hermitian, $*$-even, and $*$-odd. Numerical experiments suggest that in many cases there is a significant difference between the backward errors with respect to  perturbations that preserve structure and those with respect to arbitrary perturbations.</rss:description>
      <dc:title>Structured Eigenvalue Backward Errors of Matrix Pencils and Polynomials with Hermitian and Related Structures</dc:title>
      <dc:identifier>doi:10.1137/130925621</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 17 Apr 2014 07:00:00 GMT</dc:date>Shreemayee Bora, Michael Karow, Christian Mehl, and Punit Sharma<prism:publicationName>Structured Eigenvalue Backward Errors of Matrix Pencils and Polynomials with Hermitian and Related Structures</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>2</prism:number>
      <prism:startingPage>453</prism:startingPage>
      <prism:endingPage>475</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130925621</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130925621?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/120898784?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Adaptive Tangential Interpolation in Rational Krylov Subspaces for MIMO Dynamical Systems</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/120898784?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/2"&gt;Volume 35, Issue 2&lt;/a&gt;, Page 476-498, January 2014. &lt;br/&gt; Model reduction approaches have been shown to be powerful techniques in the numerical simulation of very large dynamical systems. The presence of multiple inputs and outputs (MIMO systems) makes the reduction process even more challenging. We consider projection-based approaches where the reduction of complexity is achieved by direct projection of the problem onto a rational Krylov subspace of significantly smaller dimension. We present an effective way to treat multiple inputs by dynamically  choosing the next direction vectors to expand the space. We apply the new strategy to the approximation of the transfer matrix function and to the solution of the Lyapunov matrix equation. Numerical results confirm that the new approach is competitive with respect to state-of-the-art methods both in terms of CPU time and memory requirements.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 2, Page 476-498, January 2014. &lt;br/&gt; Model reduction approaches have been shown to be powerful techniques in the numerical simulation of very large dynamical systems. The presence of multiple inputs and outputs (MIMO systems) makes the reduction process even more challenging. We consider projection-based approaches where the reduction of complexity is achieved by direct projection of the problem onto a rational Krylov subspace of significantly smaller dimension. We present an effective way to treat multiple inputs by dynamically  choosing the next direction vectors to expand the space. We apply the new strategy to the approximation of the transfer matrix function and to the solution of the Lyapunov matrix equation. Numerical results confirm that the new approach is competitive with respect to state-of-the-art methods both in terms of CPU time and memory requirements.</rss:description>
      <dc:title>Adaptive Tangential Interpolation in Rational Krylov Subspaces for MIMO Dynamical Systems</dc:title>
      <dc:identifier>doi:10.1137/120898784</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 17 Apr 2014 07:00:00 GMT</dc:date>V. Druskin, V. Simoncini, and M. Zaslavsky<prism:publicationName>Adaptive Tangential Interpolation in Rational Krylov Subspaces for MIMO Dynamical Systems</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>2</prism:number>
      <prism:startingPage>476</prism:startingPage>
      <prism:endingPage>498</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/120898784</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/120898784?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130921234?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>A Backward Stable Algorithm for Quadratic Eigenvalue Problems</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130921234?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/2"&gt;Volume 35, Issue 2&lt;/a&gt;, Page 499-516, January 2014. &lt;br/&gt; We present a backward stable algorithm for dense quadratic eigenvalue problems. Our algorithm  incorporates a tropical-like scaling, a strategy for choosing linearizations, and an associated strategy for recovering eigentriples. We prove that the growth factor in the translation from conditioning for the quadratic to conditioning for the linearization and the growth factor in the translation from backward error for the linearization to backward error for the quadratic are both of order one in the algorithm.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 2, Page 499-516, January 2014. &lt;br/&gt; We present a backward stable algorithm for dense quadratic eigenvalue problems. Our algorithm  incorporates a tropical-like scaling, a strategy for choosing linearizations, and an associated strategy for recovering eigentriples. We prove that the growth factor in the translation from conditioning for the quadratic to conditioning for the linearization and the growth factor in the translation from backward error for the linearization to backward error for the quadratic are both of order one in the algorithm.</rss:description>
      <dc:title>A Backward Stable Algorithm for Quadratic Eigenvalue Problems</dc:title>
      <dc:identifier>doi:10.1137/130921234</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 24 Apr 2014 07:00:00 GMT</dc:date>Linghui Zeng and Yangfeng Su<prism:publicationName>A Backward Stable Algorithm for Quadratic Eigenvalue Problems</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>2</prism:number>
      <prism:startingPage>499</prism:startingPage>
      <prism:endingPage>516</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130921234</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130921234?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130920897?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>On the Eigenvalues and Eigenvectors of Block Triangular Preconditioned Block Matrices</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130920897?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/2"&gt;Volume 35, Issue 2&lt;/a&gt;, Page 517-525, January 2014. &lt;br/&gt; Block lower triangular matrices and block upper triangular matrices are popular preconditioners for $2\times 2$ block matrices. In this note we show that a block lower triangular preconditioner gives the same spectrum as a block upper triangular preconditioner and that the eigenvectors of the two preconditioned matrices are related.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 2, Page 517-525, January 2014. &lt;br/&gt; Block lower triangular matrices and block upper triangular matrices are popular preconditioners for $2\times 2$ block matrices. In this note we show that a block lower triangular preconditioner gives the same spectrum as a block upper triangular preconditioner and that the eigenvectors of the two preconditioned matrices are related.</rss:description>
      <dc:title>On the Eigenvalues and Eigenvectors of Block Triangular Preconditioned Block Matrices</dc:title>
      <dc:identifier>doi:10.1137/130920897</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 24 Apr 2014 07:00:00 GMT</dc:date>Jennifer Pestana<prism:publicationName>On the Eigenvalues and Eigenvectors of Block Triangular Preconditioned Block Matrices</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>2</prism:number>
      <prism:startingPage>517</prism:startingPage>
      <prism:endingPage>525</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130920897</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130920897?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/120897687?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Properties of a Unitary Matrix Obtained from a Sequence of Normalized Vectors</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/120897687?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/2"&gt;Volume 35, Issue 2&lt;/a&gt;, Page 526-545, January 2014. &lt;br/&gt; In [SIAM  J. Matrix Anal. Appl., 31 (2009), pp. 565--583] it was shown how a special $(n\!+\!k)\times (n\!+\!k)$ unitary matrix can be defined from any sequence of $k$ vectors in $\mathbb{C}^n$ having  unit Euclidean norms. This unitary matrix can be called an augmented orthogonal matrix when applied in the analysis of any algorithm that seeks to compute $k$ orthonormal $n$-vectors, but where the computed, then theoretically normalized, vectors $v_j$ in $V_k=[v_1,\ldots,v_k]$ have a significant loss of orthogonality. These unitary matrices can occur in other situations, being in fact products of $k$ particular Householder matrices (unitary elementary Hermitians), and they have many interesting theoretical properties. Several new results concerning them have been collected here so that they can be easily referenced, our main purpose being to facilitate the rounding error analyses of iterative orthogonalization algorithms which lose significant orthogonality, such as the Lanczos process and its many related procedures. A key component of the analysis is the $k\times k$ strictly upper triangular matrix $S_k$ arising from $V_k$. The singular value decomposition of $S_k$ reveals the CS decomposition of the $(n\!+\!k)\times (n\!+\!k)$ unitary matrix, the null space of $V_k$, and properties of the orthogonality and loss of orthogonality resulting from its columns. Among other things these properties are used to analyze the passage towards a complete set of orthonormal vectors in $\mathbb{C}^n$ and the contribution to orthogonality of any subsequent unit norm vector $v_{k+1}$.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 2, Page 526-545, January 2014. &lt;br/&gt; In [SIAM  J. Matrix Anal. Appl., 31 (2009), pp. 565--583] it was shown how a special $(n\!+\!k)\times (n\!+\!k)$ unitary matrix can be defined from any sequence of $k$ vectors in $\mathbb{C}^n$ having  unit Euclidean norms. This unitary matrix can be called an augmented orthogonal matrix when applied in the analysis of any algorithm that seeks to compute $k$ orthonormal $n$-vectors, but where the computed, then theoretically normalized, vectors $v_j$ in $V_k=[v_1,\ldots,v_k]$ have a significant loss of orthogonality. These unitary matrices can occur in other situations, being in fact products of $k$ particular Householder matrices (unitary elementary Hermitians), and they have many interesting theoretical properties. Several new results concerning them have been collected here so that they can be easily referenced, our main purpose being to facilitate the rounding error analyses of iterative orthogonalization algorithms which lose significant orthogonality, such as the Lanczos process and its many related procedures. A key component of the analysis is the $k\times k$ strictly upper triangular matrix $S_k$ arising from $V_k$. The singular value decomposition of $S_k$ reveals the CS decomposition of the $(n\!+\!k)\times (n\!+\!k)$ unitary matrix, the null space of $V_k$, and properties of the orthogonality and loss of orthogonality resulting from its columns. Among other things these properties are used to analyze the passage towards a complete set of orthonormal vectors in $\mathbb{C}^n$ and the contribution to orthogonality of any subsequent unit norm vector $v_{k+1}$.</rss:description>
      <dc:title>Properties of a Unitary Matrix Obtained from a Sequence of Normalized Vectors</dc:title>
      <dc:identifier>doi:10.1137/120897687</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 01 May 2014 07:00:00 GMT</dc:date>Christopher C. Paige and Wolfgang Wülling<prism:publicationName>Properties of a Unitary Matrix Obtained from a Sequence of Normalized Vectors</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>2</prism:number>
      <prism:startingPage>526</prism:startingPage>
      <prism:endingPage>545</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/120897687</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/120897687?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130911032?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>On the Convergence of the Self-Consistent Field Iteration in Kohn--Sham Density Functional Theory</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130911032?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/2"&gt;Volume 35, Issue 2&lt;/a&gt;, Page 546-558, January 2014. &lt;br/&gt; It is well known that the self-consistent field (SCF) iteration for solving the  Kohn--Sham (KS) equation often fails to converge, yet there is no clear explanation. In this paper, we investigate the SCF iteration from the perspective of minimizing the corresponding KS total energy functional.  By analyzing  the second-order Taylor expansion of the KS total energy functional and estimating the relationship between the Hamiltonian and the part of the Hessian which is not used in the SCF iteration, we are able to prove global convergence from an arbitrary initial point and local linear convergence from an initial point sufficiently close to the solution of the KS equation under the assumptions that the gap between the occupied states and unoccupied states is sufficiently large  and the second-order derivatives of the exchange correlation functional are uniformly bounded from above. Although these conditions are very stringent and are almost never satisfied in reality, our analysis is interesting in the sense that it provides a qualitative prediction of the behavior of the SCF iteration.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 2, Page 546-558, January 2014. &lt;br/&gt; It is well known that the self-consistent field (SCF) iteration for solving the  Kohn--Sham (KS) equation often fails to converge, yet there is no clear explanation. In this paper, we investigate the SCF iteration from the perspective of minimizing the corresponding KS total energy functional.  By analyzing  the second-order Taylor expansion of the KS total energy functional and estimating the relationship between the Hamiltonian and the part of the Hessian which is not used in the SCF iteration, we are able to prove global convergence from an arbitrary initial point and local linear convergence from an initial point sufficiently close to the solution of the KS equation under the assumptions that the gap between the occupied states and unoccupied states is sufficiently large  and the second-order derivatives of the exchange correlation functional are uniformly bounded from above. Although these conditions are very stringent and are almost never satisfied in reality, our analysis is interesting in the sense that it provides a qualitative prediction of the behavior of the SCF iteration.</rss:description>
      <dc:title>On the Convergence of the Self-Consistent Field Iteration in Kohn--Sham Density Functional Theory</dc:title>
      <dc:identifier>doi:10.1137/130911032</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 01 May 2014 07:00:00 GMT</dc:date>Xin Liu, Xiao Wang, Zaiwen Wen, and Yaxiang Yuan<prism:publicationName>On the Convergence of the Self-Consistent Field Iteration in Kohn--Sham Density Functional Theory</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>2</prism:number>
      <prism:startingPage>546</prism:startingPage>
      <prism:endingPage>558</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130911032</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130911032?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130935665?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>On Deflations in Extended QR Algorithms</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130935665?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/2"&gt;Volume 35, Issue 2&lt;/a&gt;, Page 559-579, January 2014. &lt;br/&gt; In this paper we discuss the deflation criterion used in the extended QR algorithm based on the chasing of rotations. We provide absolute and relative perturbation bounds for this deflation criterion. Further, we present a generalization of aggressive early deflation to the extended QR algorithms.  Aggressive early deflation is the key technique for the identification and deflation of already converged, but hidden, eigenvalues. Often these possibilities for deflation are not detected by the standard technique. We present numerical results underpinning the power of aggressive early deflation also in the context of extended QR algorithms.  We further generalize these ideas by the transcription of middle deflations.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 2, Page 559-579, January 2014. &lt;br/&gt; In this paper we discuss the deflation criterion used in the extended QR algorithm based on the chasing of rotations. We provide absolute and relative perturbation bounds for this deflation criterion. Further, we present a generalization of aggressive early deflation to the extended QR algorithms.  Aggressive early deflation is the key technique for the identification and deflation of already converged, but hidden, eigenvalues. Often these possibilities for deflation are not detected by the standard technique. We present numerical results underpinning the power of aggressive early deflation also in the context of extended QR algorithms.  We further generalize these ideas by the transcription of middle deflations.</rss:description>
      <dc:title>On Deflations in Extended QR Algorithms</dc:title>
      <dc:identifier>doi:10.1137/130935665</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Tue, 06 May 2014 07:00:00 GMT</dc:date>Thomas Mach and Raf Vandebril<prism:publicationName>On Deflations in Extended QR Algorithms</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>2</prism:number>
      <prism:startingPage>559</prism:startingPage>
      <prism:endingPage>579</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130935665</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130935665?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130916710?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Orthogonal Invariance and Identifiability</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130916710?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/2"&gt;Volume 35, Issue 2&lt;/a&gt;, Page 580-598, January 2014. &lt;br/&gt; Matrix variables are ubiquitous in modern optimization, in part because variational properties of useful matrix functions often expedite standard optimization algorithms. Convexity is one important such property:  permutation-invariant convex functions of the eigenvalues of a symmetric matrix are convex, leading to the wide applicability of semidefinite programming algorithms.  We prove the analogous result for the property of “identifiability,” a notion central to many active-set-type optimization algorithms.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 2, Page 580-598, January 2014. &lt;br/&gt; Matrix variables are ubiquitous in modern optimization, in part because variational properties of useful matrix functions often expedite standard optimization algorithms. Convexity is one important such property:  permutation-invariant convex functions of the eigenvalues of a symmetric matrix are convex, leading to the wide applicability of semidefinite programming algorithms.  We prove the analogous result for the property of “identifiability,” a notion central to many active-set-type optimization algorithms.</rss:description>
      <dc:title>Orthogonal Invariance and Identifiability</dc:title>
      <dc:identifier>doi:10.1137/130916710</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Tue, 13 May 2014 07:00:00 GMT</dc:date>A. Daniilidis, D. Drusvyatskiy, and A. S. Lewis<prism:publicationName>Orthogonal Invariance and Identifiability</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>2</prism:number>
      <prism:startingPage>580</prism:startingPage>
      <prism:endingPage>598</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130916710</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130916710?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130912372?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>On a Perturbation Bound for Invariant Subspaces of Matrices</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130912372?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/2"&gt;Volume 35, Issue 2&lt;/a&gt;, Page 599-618, January 2014. &lt;br/&gt; Given a nonsymmetric matrix $A$, we investigate the effect of perturbations on an invariant subspace of $A$. The result derived in this paper differs from Stewart's classical result and sometimes yields tighter bounds. Moreover, we provide norm estimates for the remainder terms in well-known perturbation expansions for invariant subspaces, eigenvectors, and eigenvalues.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 2, Page 599-618, January 2014. &lt;br/&gt; Given a nonsymmetric matrix $A$, we investigate the effect of perturbations on an invariant subspace of $A$. The result derived in this paper differs from Stewart's classical result and sometimes yields tighter bounds. Moreover, we provide norm estimates for the remainder terms in well-known perturbation expansions for invariant subspaces, eigenvectors, and eigenvalues.</rss:description>
      <dc:title>On a Perturbation Bound for Invariant Subspaces of Matrices</dc:title>
      <dc:identifier>doi:10.1137/130912372</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Tue, 13 May 2014 07:00:00 GMT</dc:date>Michael Karow and Daniel Kressner<prism:publicationName>On a Perturbation Bound for Invariant Subspaces of Matrices</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>2</prism:number>
      <prism:startingPage>599</prism:startingPage>
      <prism:endingPage>618</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130912372</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130912372?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130933228?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Calculating the $H_{\infty}$-norm Using the Implicit Determinant Method</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130933228?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/2"&gt;Volume 35, Issue 2&lt;/a&gt;, Page 619-635, January 2014. &lt;br/&gt; We propose a fast algorithm to calculate the $H_{\infty}$-norm of a transfer matrix. The method builds on a well-known relationship between singular values of the transfer function and pure imaginary eigenvalues of a certain Hamiltonian matrix. Using this property we construct a two-parameter eigenvalue problem, where, in the generic case, the critical value corresponds to a two-dimensional Jordan block. We use the implicit determinant method which replaces the need for eigensolves by the solution of linear systems, a technique recently used in [M. A. Freitag and A. Spence, Linear Algebra Appl., 435 (2011), pp. 3189--3205] for finding the distance to instability. In this paper the method takes advantage of the structured linear systems that arise within the algorithm to obtain efficient solves using the staircase reduction. We give numerical examples and compare our method to the algorithm proposed in [N. Guglielmi, M. Gürbüzbalaban, and M. L. Overton, SIAM J. Matrix Anal. Appl., 34 (2013), pp. 709--737].</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 2, Page 619-635, January 2014. &lt;br/&gt; We propose a fast algorithm to calculate the $H_{\infty}$-norm of a transfer matrix. The method builds on a well-known relationship between singular values of the transfer function and pure imaginary eigenvalues of a certain Hamiltonian matrix. Using this property we construct a two-parameter eigenvalue problem, where, in the generic case, the critical value corresponds to a two-dimensional Jordan block. We use the implicit determinant method which replaces the need for eigensolves by the solution of linear systems, a technique recently used in [M. A. Freitag and A. Spence, Linear Algebra Appl., 435 (2011), pp. 3189--3205] for finding the distance to instability. In this paper the method takes advantage of the structured linear systems that arise within the algorithm to obtain efficient solves using the staircase reduction. We give numerical examples and compare our method to the algorithm proposed in [N. Guglielmi, M. Gürbüzbalaban, and M. L. Overton, SIAM J. Matrix Anal. Appl., 34 (2013), pp. 709--737].</rss:description>
      <dc:title>Calculating the $H_{\infty}$-norm Using the Implicit Determinant Method</dc:title>
      <dc:identifier>doi:10.1137/130933228</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 15 May 2014 07:00:00 GMT</dc:date>Melina A. Freitag, Alastair Spence, and Paul Van Dooren<prism:publicationName>Calculating the $H_{\infty}$-norm Using the Implicit Determinant Method</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>2</prism:number>
      <prism:startingPage>619</prism:startingPage>
      <prism:endingPage>635</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130933228</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130933228?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130916084?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Canonical Polyadic Decomposition of Third-Order Tensors: Reduction to Generalized Eigenvalue Decomposition</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130916084?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/2"&gt;Volume 35, Issue 2&lt;/a&gt;, Page 636-660, January 2014. &lt;br/&gt; Canonical polyadic decomposition (CPD) of a third-order tensor is  decomposition in a minimal number of rank-1 tensors. We call an algorithm algebraic if it is guaranteed to find the decomposition when it is exact and if it relies only on standard linear algebra (essentially sets of linear equations and matrix factorizations). The known algebraic algorithms for the  computation of the CPD are limited to cases where at least one of the factor matrices  has full column rank. In this paper we present an algebraic algorithm for the computation of the CPD in cases where none of the factor matrices has full column rank. In particular, we show that if the famous Kruskal condition holds, then the CPD can be found algebraically.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 2, Page 636-660, January 2014. &lt;br/&gt; Canonical polyadic decomposition (CPD) of a third-order tensor is  decomposition in a minimal number of rank-1 tensors. We call an algorithm algebraic if it is guaranteed to find the decomposition when it is exact and if it relies only on standard linear algebra (essentially sets of linear equations and matrix factorizations). The known algebraic algorithms for the  computation of the CPD are limited to cases where at least one of the factor matrices  has full column rank. In this paper we present an algebraic algorithm for the computation of the CPD in cases where none of the factor matrices has full column rank. In particular, we show that if the famous Kruskal condition holds, then the CPD can be found algebraically.</rss:description>
      <dc:title>Canonical Polyadic Decomposition of Third-Order Tensors: Reduction to Generalized Eigenvalue Decomposition</dc:title>
      <dc:identifier>doi:10.1137/130916084</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 15 May 2014 07:00:00 GMT</dc:date>Ignat Domanov and Lieven De Lathauwer<prism:publicationName>Canonical Polyadic Decomposition of Third-Order Tensors: Reduction to Generalized Eigenvalue Decomposition</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>2</prism:number>
      <prism:startingPage>636</prism:startingPage>
      <prism:endingPage>660</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130916084</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130916084?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/13093491X?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Efficient and Stable Arnoldi Restarts for Matrix Functions Based on Quadrature</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/13093491X?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/2"&gt;Volume 35, Issue 2&lt;/a&gt;, Page 661-683, January 2014. &lt;br/&gt; When using the Arnoldi method for approximating $f(A){\mathbf b}$, the action of a matrix function on a vector, the maximum number of iterations that can be performed is often limited by the storage requirements of the full Arnoldi basis. As a remedy, different restarting algorithms have been proposed in the literature, none of which has been universally applicable, efficient, and stable at the same time. We utilize an integral representation for the error of the iterates in the Arnoldi method which then allows us to develop an efficient quadrature-based restarting algorithm suitable for a large class of functions, including the so-called Stieltjes functions and the exponential function. Our method is applicable for functions of Hermitian and non-Hermitian matrices, requires no a priori spectral information, and runs with essentially constant computational work per restart cycle.  We comment on the relation of this new restarting approach to other existing algorithms and illustrate its efficiency and numerical stability by various numerical experiments.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 2, Page 661-683, January 2014. &lt;br/&gt; When using the Arnoldi method for approximating $f(A){\mathbf b}$, the action of a matrix function on a vector, the maximum number of iterations that can be performed is often limited by the storage requirements of the full Arnoldi basis. As a remedy, different restarting algorithms have been proposed in the literature, none of which has been universally applicable, efficient, and stable at the same time. We utilize an integral representation for the error of the iterates in the Arnoldi method which then allows us to develop an efficient quadrature-based restarting algorithm suitable for a large class of functions, including the so-called Stieltjes functions and the exponential function. Our method is applicable for functions of Hermitian and non-Hermitian matrices, requires no a priori spectral information, and runs with essentially constant computational work per restart cycle.  We comment on the relation of this new restarting approach to other existing algorithms and illustrate its efficiency and numerical stability by various numerical experiments.</rss:description>
      <dc:title>Efficient and Stable Arnoldi Restarts for Matrix Functions Based on Quadrature</dc:title>
      <dc:identifier>doi:10.1137/13093491X</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 22 May 2014 07:00:00 GMT</dc:date>Andreas Frommer, Stefan Güttel, and Marcel Schweitzer<prism:publicationName>Efficient and Stable Arnoldi Restarts for Matrix Functions Based on Quadrature</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>2</prism:number>
      <prism:startingPage>661</prism:startingPage>
      <prism:endingPage>683</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/13093491X</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/13093491X?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130927231?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Improved Backward Error Bounds for LU and Cholesky Factorizations</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130927231?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/2"&gt;Volume 35, Issue 2&lt;/a&gt;, Page 684-698, January 2014. &lt;br/&gt; Assuming standard floating-point arithmetic (in base $\beta$, precision $p$) and barring underflow and overflow, classical rounding error analysis of the LU or Cholesky factorization of an $n\times n$ matrix $A$ provides backward error bounds of the form $|\Delta A| \le \gamma_n |\widehat L| |\widehat U|$ or $|\Delta A| \le \gamma_{n+1} |\widehat R^T| |\widehat R|$. Here, $\widehat L$, $\widehat U$, and $\widehat R$ denote the computed factors, and $\gamma_n$ is the usual fraction $nu/(1-nu) = nu + {\mathcal O}(u^2)$ with $u$ the unit roundoff. Similarly, when solving an $n\times n$ triangular system $Tx = b$ by substitution, the computed solution $\widehat x$ satisfies $(T+\Delta T)\widehat x = b$ with $|\Delta T| \le \gamma_n |T|$. All these error bounds contain quadratic terms in $u$ and limit $n$ to satisfy either $nu&amp;lt;1$ or $(n+1)u &amp;lt; 1$. We show in this paper that the constants $\gamma_n$ and $\gamma_{n+1}$ can be replaced by $nu$ and $(n+1)u$, respectively, and that the restrictions on $n$ can be removed. To get these new bounds the main ingredient is a general framework for bounding expressions of the form $|\rho-s|$, where $s$ is the exact sum of a floating-point number and $n-1$ real numbers and where $\rho$ is a real number approximating the computed sum $\widehat s$. By instantiating this framework with suitable values of $\rho$, we obtain improved versions of the well-known Lemma 8.4 from [N. J. Higham, Accuracy and Stability of Numerical Algorithms, 2nd ed., SIAM, Philadelphia, 2002] (used for analyzing triangular system solving and LU factorization) and of its Cholesky variant. All our results hold for rounding to nearest with any tie-breaking strategy and whatever the order of summation.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 2, Page 684-698, January 2014. &lt;br/&gt; Assuming standard floating-point arithmetic (in base $\beta$, precision $p$) and barring underflow and overflow, classical rounding error analysis of the LU or Cholesky factorization of an $n\times n$ matrix $A$ provides backward error bounds of the form $|\Delta A| \le \gamma_n |\widehat L| |\widehat U|$ or $|\Delta A| \le \gamma_{n+1} |\widehat R^T| |\widehat R|$. Here, $\widehat L$, $\widehat U$, and $\widehat R$ denote the computed factors, and $\gamma_n$ is the usual fraction $nu/(1-nu) = nu + {\mathcal O}(u^2)$ with $u$ the unit roundoff. Similarly, when solving an $n\times n$ triangular system $Tx = b$ by substitution, the computed solution $\widehat x$ satisfies $(T+\Delta T)\widehat x = b$ with $|\Delta T| \le \gamma_n |T|$. All these error bounds contain quadratic terms in $u$ and limit $n$ to satisfy either $nu&lt;1$ or $(n+1)u &lt; 1$. We show in this paper that the constants $\gamma_n$ and $\gamma_{n+1}$ can be replaced by $nu$ and $(n+1)u$, respectively, and that the restrictions on $n$ can be removed. To get these new bounds the main ingredient is a general framework for bounding expressions of the form $|\rho-s|$, where $s$ is the exact sum of a floating-point number and $n-1$ real numbers and where $\rho$ is a real number approximating the computed sum $\widehat s$. By instantiating this framework with suitable values of $\rho$, we obtain improved versions of the well-known Lemma 8.4 from [N. J. Higham, Accuracy and Stability of Numerical Algorithms, 2nd ed., SIAM, Philadelphia, 2002] (used for analyzing triangular system solving and LU factorization) and of its Cholesky variant. All our results hold for rounding to nearest with any tie-breaking strategy and whatever the order of summation.</rss:description>
      <dc:title>Improved Backward Error Bounds for LU and Cholesky Factorizations</dc:title>
      <dc:identifier>doi:10.1137/130927231</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Tue, 27 May 2014 07:00:00 GMT</dc:date>Siegfried M. Rump and Claude-Pierre Jeannerod<prism:publicationName>Improved Backward Error Bounds for LU and Cholesky Factorizations</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>2</prism:number>
      <prism:startingPage>684</prism:startingPage>
      <prism:endingPage>698</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130927231</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130927231?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130933472?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Numerical Optimization of Eigenvalues of Hermitian Matrix Functions</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130933472?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/2"&gt;Volume 35, Issue 2&lt;/a&gt;, Page 699-724, January 2014. &lt;br/&gt; This work concerns the global minimization of a prescribed eigenvalue or a weighted sum of prescribed eigenvalues of a Hermitian matrix-valued function depending on its parameters analytically in a box. We describe how the analytical properties of eigenvalue functions can be put into use to derive piecewise quadratic functions that underestimate the eigenvalue functions. These piecewise quadratic underestimators lead us to a global minimization algorithm, originally due to Breiman and Cutler. We prove the global convergence of the algorithm and show that it can be effectively used for the minimization of extreme eigenvalues, e.g., the largest eigenvalue or the sum of the largest specified number of eigenvalues. This is particularly facilitated by the analytical formulas for the first derivatives of eigenvalues, as well as analytical lower bounds on the second derivatives that can be deduced for extreme eigenvalue functions. The applications that we have in mind also include the ${\rm H}_\infty$-norm of a linear dynamical system, numerical radius, distance to uncontrollability, and various other nonconvex eigenvalue optimization problems, for which, generically, the eigenvalue function involved is simple at all points.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 2, Page 699-724, January 2014. &lt;br/&gt; This work concerns the global minimization of a prescribed eigenvalue or a weighted sum of prescribed eigenvalues of a Hermitian matrix-valued function depending on its parameters analytically in a box. We describe how the analytical properties of eigenvalue functions can be put into use to derive piecewise quadratic functions that underestimate the eigenvalue functions. These piecewise quadratic underestimators lead us to a global minimization algorithm, originally due to Breiman and Cutler. We prove the global convergence of the algorithm and show that it can be effectively used for the minimization of extreme eigenvalues, e.g., the largest eigenvalue or the sum of the largest specified number of eigenvalues. This is particularly facilitated by the analytical formulas for the first derivatives of eigenvalues, as well as analytical lower bounds on the second derivatives that can be deduced for extreme eigenvalue functions. The applications that we have in mind also include the ${\rm H}_\infty$-norm of a linear dynamical system, numerical radius, distance to uncontrollability, and various other nonconvex eigenvalue optimization problems, for which, generically, the eigenvalue function involved is simple at all points.</rss:description>
      <dc:title>Numerical Optimization of Eigenvalues of Hermitian Matrix Functions</dc:title>
      <dc:identifier>doi:10.1137/130933472</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 05 Jun 2014 07:00:00 GMT</dc:date>Emre Mengi, E. Alper Yildirim, and Mustafa Kiliç<prism:publicationName>Numerical Optimization of Eigenvalues of Hermitian Matrix Functions</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>2</prism:number>
      <prism:startingPage>699</prism:startingPage>
      <prism:endingPage>724</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130933472</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130933472?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/120902677?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>A Fast Semidirect Least Squares Algorithm for Hierarchically Block Separable Matrices</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/120902677?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/2"&gt;Volume 35, Issue 2&lt;/a&gt;, Page 725-748, January 2014. &lt;br/&gt; We present a fast algorithm for linear least squares problems governed by hierarchically block separable (HBS) matrices. Such matrices are generally dense but data sparse and can describe many important operators including those derived from asymptotically smooth radial kernels that are not too oscillatory. The algorithm is based on a recursive skeletonization procedure that exposes this sparsity and solves the dense least squares problem as a larger, equality-constrained, sparse one. It relies on a sparse QR factorization coupled with iterative weighted least squares methods. In essence, our scheme consists of a direct component, comprised of matrix compression and factorization, followed by an iterative component to enforce certain equality constraints. At most two iterations are typically required for problems that are not too ill-conditioned. For an $M \times N$ HBS matrix with $M \geq N$ having bounded off-diagonal block rank, the algorithm has optimal $\mathcal{O} (M + N)$ complexity. If the rank increases with the spatial dimension as is common for operators that are singular at the origin, then this becomes $\mathcal{O} (M + N)$ in one dimension, $\mathcal{O} (M +  N^{3/2})$ in two dimensions, and $\mathcal{O} (M + N^{2})$ in three dimensions. We illustrate the performance of the method on both overdetermined and underdetermined systems in a variety of settings, with an emphasis on radial basis function approximation and efficient updating and downdating.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 2, Page 725-748, January 2014. &lt;br/&gt; We present a fast algorithm for linear least squares problems governed by hierarchically block separable (HBS) matrices. Such matrices are generally dense but data sparse and can describe many important operators including those derived from asymptotically smooth radial kernels that are not too oscillatory. The algorithm is based on a recursive skeletonization procedure that exposes this sparsity and solves the dense least squares problem as a larger, equality-constrained, sparse one. It relies on a sparse QR factorization coupled with iterative weighted least squares methods. In essence, our scheme consists of a direct component, comprised of matrix compression and factorization, followed by an iterative component to enforce certain equality constraints. At most two iterations are typically required for problems that are not too ill-conditioned. For an $M \times N$ HBS matrix with $M \geq N$ having bounded off-diagonal block rank, the algorithm has optimal $\mathcal{O} (M + N)$ complexity. If the rank increases with the spatial dimension as is common for operators that are singular at the origin, then this becomes $\mathcal{O} (M + N)$ in one dimension, $\mathcal{O} (M +  N^{3/2})$ in two dimensions, and $\mathcal{O} (M + N^{2})$ in three dimensions. We illustrate the performance of the method on both overdetermined and underdetermined systems in a variety of settings, with an emphasis on radial basis function approximation and efficient updating and downdating.</rss:description>
      <dc:title>A Fast Semidirect Least Squares Algorithm for Hierarchically Block Separable Matrices</dc:title>
      <dc:identifier>doi:10.1137/120902677</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 05 Jun 2014 07:00:00 GMT</dc:date>Kenneth L. Ho and Leslie Greengard<prism:publicationName>A Fast Semidirect Least Squares Algorithm for Hierarchically Block Separable Matrices</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>2</prism:number>
      <prism:startingPage>725</prism:startingPage>
      <prism:endingPage>748</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/120902677</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/120902677?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130935537?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Rank-One Corrections of Nonnegative Matrices, With an Application to Matrix Population Models</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130935537?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/2"&gt;Volume 35, Issue 2&lt;/a&gt;, Page 749-764, January 2014. &lt;br/&gt; We study the location of  $\lambda_2 (A)$, the second positive eigenvalue of a nonnegative matrix $A$, as the issue of how many positive eigenvalues can be shifted beyond the spectral radius $\rho(A)$ by means of arbitrary changes in elements of one row. The notion of rank-one correction suggests the nearest generalization expanding the changes in one row to any matrix of rank one (still keeping the matrix nonnegative). The main theorem limits the number of those eigenvalues, counting multiplicities, to the increased spectral radius alone. In matrix population models, we treat the projection matrix $L = T + F$ as the rank-one correction of its transition part $T$ by the fertility one $F$. The matrix $T$ is column substochastic due to its demographic interpretation, hence we conclude that  $\lambda_2(L) \le 1$ and specify the rare cases where  $\lambda_2(L) = 1$. The location  $\lambda_2(L) &amp;lt; 1$ ensures that the function $R(L) = 1 - {det}~ (I - L)$ has the indicator property, namely, its value is always located on the same side of 1 as is $\rho (L)$. This  indicator does not pose any computational problems and helps calibrate $L$ from empirical data.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 2, Page 749-764, January 2014. &lt;br/&gt; We study the location of  $\lambda_2 (A)$, the second positive eigenvalue of a nonnegative matrix $A$, as the issue of how many positive eigenvalues can be shifted beyond the spectral radius $\rho(A)$ by means of arbitrary changes in elements of one row. The notion of rank-one correction suggests the nearest generalization expanding the changes in one row to any matrix of rank one (still keeping the matrix nonnegative). The main theorem limits the number of those eigenvalues, counting multiplicities, to the increased spectral radius alone. In matrix population models, we treat the projection matrix $L = T + F$ as the rank-one correction of its transition part $T$ by the fertility one $F$. The matrix $T$ is column substochastic due to its demographic interpretation, hence we conclude that  $\lambda_2(L) \le 1$ and specify the rare cases where  $\lambda_2(L) = 1$. The location  $\lambda_2(L) &lt; 1$ ensures that the function $R(L) = 1 - {det}~ (I - L)$ has the indicator property, namely, its value is always located on the same side of 1 as is $\rho (L)$. This  indicator does not pose any computational problems and helps calibrate $L$ from empirical data.</rss:description>
      <dc:title>Rank-One Corrections of Nonnegative Matrices, With an Application to Matrix Population Models</dc:title>
      <dc:identifier>doi:10.1137/130935537</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Tue, 10 Jun 2014 07:00:00 GMT</dc:date>Vladimir Yu. Protasov and Dmitrii O. Logofet<prism:publicationName>Rank-One Corrections of Nonnegative Matrices, With an Application to Matrix Population Models</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>2</prism:number>
      <prism:startingPage>749</prism:startingPage>
      <prism:endingPage>764</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130935537</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130935537?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130946563?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Rayleigh--Ritz Approximation For the Linear Response Eigenvalue Problem</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130946563?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/2"&gt;Volume 35, Issue 2&lt;/a&gt;, Page 765-782, January 2014. &lt;br/&gt; Large scale eigenvalue computation is about approximating certain invariant subspaces associated with the interesting part of the spectrum, and the interesting eigenvalues are then extracted through projecting the problem through approximate invariant subspaces into a much smaller eigenvalue problem. In the case of the linear response eigenvalue problem (aka the random phase eigenvalue problem), it is the pair of deflating subspaces associated with the first few smallest positive eigenvalues that needs to be computed. This paper is concerned with approximation accuracy relationships between a pair of approximate deflating subspaces and approximate eigenvalues extracted by the pair. Lower and upper bounds on eigenvalue approximation errors are obtained in terms of canonical angles between an exact and computed pair of deflating subspaces. These bounds can also be interpreted as lower/upper bounds on the canonical angles in terms of eigenvalue approximation errors. They are useful in analyzing numerical solutions to linear response eigenvalue problems.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 2, Page 765-782, January 2014. &lt;br/&gt; Large scale eigenvalue computation is about approximating certain invariant subspaces associated with the interesting part of the spectrum, and the interesting eigenvalues are then extracted through projecting the problem through approximate invariant subspaces into a much smaller eigenvalue problem. In the case of the linear response eigenvalue problem (aka the random phase eigenvalue problem), it is the pair of deflating subspaces associated with the first few smallest positive eigenvalues that needs to be computed. This paper is concerned with approximation accuracy relationships between a pair of approximate deflating subspaces and approximate eigenvalues extracted by the pair. Lower and upper bounds on eigenvalue approximation errors are obtained in terms of canonical angles between an exact and computed pair of deflating subspaces. These bounds can also be interpreted as lower/upper bounds on the canonical angles in terms of eigenvalue approximation errors. They are useful in analyzing numerical solutions to linear response eigenvalue problems.</rss:description>
      <dc:title>Rayleigh--Ritz Approximation For the Linear Response Eigenvalue Problem</dc:title>
      <dc:identifier>doi:10.1137/130946563</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Tue, 10 Jun 2014 07:00:00 GMT</dc:date>Lei-Hong Zhang, Jungong Xue, and Ren-Cang Li<prism:publicationName>Rayleigh--Ritz Approximation For the Linear Response Eigenvalue Problem</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>2</prism:number>
      <prism:startingPage>765</prism:startingPage>
      <prism:endingPage>782</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130946563</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130946563?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130920629?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Compressed Threshold Pivoting for Sparse Symmetric Indefinite Systems</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130920629?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/2"&gt;Volume 35, Issue 2&lt;/a&gt;, Page 783-817, January 2014. &lt;br/&gt; A key technique for controlling numerical stability in sparse direct solvers is threshold partial pivoting. When selecting a pivot, the entire candidate pivot column below the diagonal must be up-to-date and must be scanned. If the factorization is parallelized across a large number of cores, communication latencies can be the dominant computational cost. In this paper, we propose two alternative pivoting strategies for sparse symmetric indefinite matrices of full rank that significantly reduce communication by compressing the necessary data into a small matrix that can be used to select pivots. Once pivots have been chosen, they can be applied in a communication-efficient fashion. For an $n\times p$ submatrix on $P$ processors, we show our methods perform a factorization using $O(\log P)$ messages instead of the $O(p\log P)$ for threshold partial pivoting. The additional costs in terms of operations and communication bandwidth are relatively small. A stability proof is given and numerical results using a range of symmetric indefinite matrices arising from practical problems are used to demonstrate the practical robustness. Timing results on large random examples illustrate the potential speedup on current multicore machines.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 2, Page 783-817, January 2014. &lt;br/&gt; A key technique for controlling numerical stability in sparse direct solvers is threshold partial pivoting. When selecting a pivot, the entire candidate pivot column below the diagonal must be up-to-date and must be scanned. If the factorization is parallelized across a large number of cores, communication latencies can be the dominant computational cost. In this paper, we propose two alternative pivoting strategies for sparse symmetric indefinite matrices of full rank that significantly reduce communication by compressing the necessary data into a small matrix that can be used to select pivots. Once pivots have been chosen, they can be applied in a communication-efficient fashion. For an $n\times p$ submatrix on $P$ processors, we show our methods perform a factorization using $O(\log P)$ messages instead of the $O(p\log P)$ for threshold partial pivoting. The additional costs in terms of operations and communication bandwidth are relatively small. A stability proof is given and numerical results using a range of symmetric indefinite matrices arising from practical problems are used to demonstrate the practical robustness. Timing results on large random examples illustrate the potential speedup on current multicore machines.</rss:description>
      <dc:title>Compressed Threshold Pivoting for Sparse Symmetric Indefinite Systems</dc:title>
      <dc:identifier>doi:10.1137/130920629</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Tue, 17 Jun 2014 07:00:00 GMT</dc:date>J. D. Hogg and J. A. Scott<prism:publicationName>Compressed Threshold Pivoting for Sparse Symmetric Indefinite Systems</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>2</prism:number>
      <prism:startingPage>783</prism:startingPage>
      <prism:endingPage>817</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130920629</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130920629?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130912438?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>On the Minimum FLOPs Problem in the Sparse Cholesky Factorization</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130912438?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/1"&gt;Volume 35, Issue 1&lt;/a&gt;, Page 1-21, January 2014. &lt;br/&gt; Prior to computing the Cholesky factorization of a sparse symmetric positive definite matrix, a reordering of the rows and columns is computed so as to reduce both the number of fill elements in Cholesky factor and the number of arithmetic operations (FLOPs) in the numerical factorization. These two metrics are clearly somehow related and yet it is suspected that these two problems are different.  However, no rigorous theoretical treatment of the relation of these two problems seems to have been given yet.  In this paper we show by means of an explicit, scalable construction that the two problems are different in a very strict sense: no ordering is optimal for both fill and FLOPs in the constructed graph. Further, it is commonly believed that minimizing the number of FLOPs is no easier than minimizing the fill (in the complexity sense), but so far no proof appears to be known.  We give a reduction chain that shows the NP hardness of minimizing the number of arithmetic operations in the Cholesky factorization.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 1, Page 1-21, January 2014. &lt;br/&gt; Prior to computing the Cholesky factorization of a sparse symmetric positive definite matrix, a reordering of the rows and columns is computed so as to reduce both the number of fill elements in Cholesky factor and the number of arithmetic operations (FLOPs) in the numerical factorization. These two metrics are clearly somehow related and yet it is suspected that these two problems are different.  However, no rigorous theoretical treatment of the relation of these two problems seems to have been given yet.  In this paper we show by means of an explicit, scalable construction that the two problems are different in a very strict sense: no ordering is optimal for both fill and FLOPs in the constructed graph. Further, it is commonly believed that minimizing the number of FLOPs is no easier than minimizing the fill (in the complexity sense), but so far no proof appears to be known.  We give a reduction chain that shows the NP hardness of minimizing the number of arithmetic operations in the Cholesky factorization.</rss:description>
      <dc:title>On the Minimum FLOPs Problem in the Sparse Cholesky Factorization</dc:title>
      <dc:identifier>doi:10.1137/130912438</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 02 Jan 2014 08:00:00 GMT</dc:date>Robert Luce and Esmond G. Ng<prism:publicationName>On the Minimum FLOPs Problem in the Sparse Cholesky Factorization</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>1</prism:number>
      <prism:startingPage>1</prism:startingPage>
      <prism:endingPage>21</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130912438</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130912438?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/120893057?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>A Residual Replacement Strategy for Improving the Maximum Attainable Accuracy of $s$-Step Krylov Subspace Methods</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/120893057?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/1"&gt;Volume 35, Issue 1&lt;/a&gt;, Page 22-43, January 2014. &lt;br/&gt; Krylov subspace methods are a popular class of iterative methods for solving linear systems with large, sparse matrices. On modern computer architectures, both sequential and parallel performance of classical Krylov methods is limited by costly data movement, or communication, required to update the approximate solution in each iteration. These motivated communication-avoiding Krylov methods, based on $s$-step formulations, reduce data movement by a factor of $O(s)$ by reordering the computations in classical Krylov methods to exploit locality. Studies on the finite precision behavior of communication-avoiding Krylov methods in the literature have thus far been empirical in nature; in this work, we provide the first quantitative analysis of the maximum attainable accuracy of communication-avoiding Krylov subspace methods in finite precision. Following the analysis for classical Krylov methods, we derive a bound on the deviation of the true and updated residuals in communication-avoiding conjugate gradient and communication-avoiding biconjugate gradient in finite precision. Furthermore, an estimate for this bound can be iteratively updated within the method without asymptotically increasing communication or computation. Our bound enables an implicit residual replacement strategy for maintaining agreement between residuals to within $O(\epsilon)\Vert A \Vert\Vert x\Vert$. Numerical experiments on a small set of test matrices verify that, for cases where the updated residual converges, the residual replacement strategy can enable accuracy of $O(\epsilon)\Vert A\Vert\Vert x\Vert$ with a small number of residual replacement steps, reflecting improvements of up to seven orders of magnitude.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 1, Page 22-43, January 2014. &lt;br/&gt; Krylov subspace methods are a popular class of iterative methods for solving linear systems with large, sparse matrices. On modern computer architectures, both sequential and parallel performance of classical Krylov methods is limited by costly data movement, or communication, required to update the approximate solution in each iteration. These motivated communication-avoiding Krylov methods, based on $s$-step formulations, reduce data movement by a factor of $O(s)$ by reordering the computations in classical Krylov methods to exploit locality. Studies on the finite precision behavior of communication-avoiding Krylov methods in the literature have thus far been empirical in nature; in this work, we provide the first quantitative analysis of the maximum attainable accuracy of communication-avoiding Krylov subspace methods in finite precision. Following the analysis for classical Krylov methods, we derive a bound on the deviation of the true and updated residuals in communication-avoiding conjugate gradient and communication-avoiding biconjugate gradient in finite precision. Furthermore, an estimate for this bound can be iteratively updated within the method without asymptotically increasing communication or computation. Our bound enables an implicit residual replacement strategy for maintaining agreement between residuals to within $O(\epsilon)\Vert A \Vert\Vert x\Vert$. Numerical experiments on a small set of test matrices verify that, for cases where the updated residual converges, the residual replacement strategy can enable accuracy of $O(\epsilon)\Vert A\Vert\Vert x\Vert$ with a small number of residual replacement steps, reflecting improvements of up to seven orders of magnitude.</rss:description>
      <dc:title>A Residual Replacement Strategy for Improving the Maximum Attainable Accuracy of $s$-Step Krylov Subspace Methods</dc:title>
      <dc:identifier>doi:10.1137/120893057</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Tue, 21 Jan 2014 08:00:00 GMT</dc:date>Erin Carson and James Demmel<prism:publicationName>A Residual Replacement Strategy for Improving the Maximum Attainable Accuracy of $s$-Step Krylov Subspace Methods</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>1</prism:number>
      <prism:startingPage>22</prism:startingPage>
      <prism:endingPage>43</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/120893057</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/120893057?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/120895755?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Superfast and Stable Structured Solvers for Toeplitz Least Squares via Randomized Sampling</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/120895755?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/1"&gt;Volume 35, Issue 1&lt;/a&gt;, Page 44-72, January 2014. &lt;br/&gt; We present some superfast ($O((m+n)\log^{2}(m+n))$ complexity) and stable structured direct solvers for $m\times n$ Toeplitz least squares problems. Based on the displacement equation, a Toeplitz matrix $T$ is first transformed into a Cauchy-like matrix $\mathcal{C}$, which can be shown to have small off-diagonal numerical ranks when the diagonal blocks are rectangular. We generalize standard hierarchically semiseparable (HSS) matrix representations to rectangular ones, and construct a rectangular HSS approximation to $\mathcal{C}$ in nearly linear complexity with randomized sampling and fast multiplications of $\mathcal{C}$ with vectors. A new URV HSS factorization and a URV HSS solution are designed for the least squares solution. We also present two structured normal equation methods. Systematic error and stability analysis for our HSS methods is given, which is also useful for studying other HSS and rank structured methods. We derive the growth factors and the backward error bounds in the HSS factorizations, and show that the stability results are generally much better than those in dense LU factorizations with partial pivoting. Such analysis has not been done before for HSS matrices. The solvers are tested on various classical Toeplitz examples ranging from well-conditioned to highly ill-conditioned ones. Comparisons with some recent fast and superfast solvers are given. Our new methods are generally much faster, and give better (or at least comparable) accuracies, especially for ill-conditioned problems.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 1, Page 44-72, January 2014. &lt;br/&gt; We present some superfast ($O((m+n)\log^{2}(m+n))$ complexity) and stable structured direct solvers for $m\times n$ Toeplitz least squares problems. Based on the displacement equation, a Toeplitz matrix $T$ is first transformed into a Cauchy-like matrix $\mathcal{C}$, which can be shown to have small off-diagonal numerical ranks when the diagonal blocks are rectangular. We generalize standard hierarchically semiseparable (HSS) matrix representations to rectangular ones, and construct a rectangular HSS approximation to $\mathcal{C}$ in nearly linear complexity with randomized sampling and fast multiplications of $\mathcal{C}$ with vectors. A new URV HSS factorization and a URV HSS solution are designed for the least squares solution. We also present two structured normal equation methods. Systematic error and stability analysis for our HSS methods is given, which is also useful for studying other HSS and rank structured methods. We derive the growth factors and the backward error bounds in the HSS factorizations, and show that the stability results are generally much better than those in dense LU factorizations with partial pivoting. Such analysis has not been done before for HSS matrices. The solvers are tested on various classical Toeplitz examples ranging from well-conditioned to highly ill-conditioned ones. Comparisons with some recent fast and superfast solvers are given. Our new methods are generally much faster, and give better (or at least comparable) accuracies, especially for ill-conditioned problems.</rss:description>
      <dc:title>Superfast and Stable Structured Solvers for Toeplitz Least Squares via Randomized Sampling</dc:title>
      <dc:identifier>doi:10.1137/120895755</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Tue, 21 Jan 2014 08:00:00 GMT</dc:date>Yuanzhe Xi, Jianlin Xia, Stephen Cauley, and Venkataramanan Balakrishnan<prism:publicationName>Superfast and Stable Structured Solvers for Toeplitz Least Squares via Randomized Sampling</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>1</prism:number>
      <prism:startingPage>44</prism:startingPage>
      <prism:endingPage>72</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/120895755</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/120895755?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/120892891?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Geometric Measure of Entanglement and U-Eigenvalues of Tensors</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/120892891?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/1"&gt;Volume 35, Issue 1&lt;/a&gt;, Page 73-87, January 2014. &lt;br/&gt; We study tensor analysis problems motivated by the geometric measure of quantum entanglement. We define the concept of the unitary eigenvalue (U-eigenvalue) of a complex tensor, the unitary symmetric eigenvalue (US-eigenvalue) of a symmetric complex tensor, and the best complex rank-one approximation. We obtain an upper bound on the number of distinct US-eigenvalues of symmetric tensors and count all US-eigenpairs with nonzero eigenvalues of symmetric tensors. We convert the geometric measure of the entanglement problem to an algebraic equation system problem. A numerical example shows that a symmetric real tensor may have a best complex rank-one approximation that is better than its best real rank-one approximation, which implies that the absolute-value largest Z-eigenvalue is not always the geometric measure of entanglement.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 1, Page 73-87, January 2014. &lt;br/&gt; We study tensor analysis problems motivated by the geometric measure of quantum entanglement. We define the concept of the unitary eigenvalue (U-eigenvalue) of a complex tensor, the unitary symmetric eigenvalue (US-eigenvalue) of a symmetric complex tensor, and the best complex rank-one approximation. We obtain an upper bound on the number of distinct US-eigenvalues of symmetric tensors and count all US-eigenpairs with nonzero eigenvalues of symmetric tensors. We convert the geometric measure of the entanglement problem to an algebraic equation system problem. A numerical example shows that a symmetric real tensor may have a best complex rank-one approximation that is better than its best real rank-one approximation, which implies that the absolute-value largest Z-eigenvalue is not always the geometric measure of entanglement.</rss:description>
      <dc:title>Geometric Measure of Entanglement and U-Eigenvalues of Tensors</dc:title>
      <dc:identifier>doi:10.1137/120892891</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 23 Jan 2014 08:00:00 GMT</dc:date>Guyan Ni, Liqun Qi, and Minru Bai<prism:publicationName>Geometric Measure of Entanglement and U-Eigenvalues of Tensors</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>1</prism:number>
      <prism:startingPage>73</prism:startingPage>
      <prism:endingPage>87</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/120892891</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/120892891?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130920137?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>The Matrix Unwinding Function, with an Application to Computing the Matrix Exponential</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130920137?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/1"&gt;Volume 35, Issue 1&lt;/a&gt;, Page 88-109, January 2014. &lt;br/&gt; A new matrix function corresponding to the scalar unwinding number of                     Corless, Hare, and Jeffrey is introduced.                     This matrix unwinding function, $\mathcal{U}$, is shown to                     be a valuable tool for deriving                     identities involving the matrix logarithm and                     fractional matrix powers,                     revealing, for example, the                     precise relation between $\log A^\alpha$ and $\alpha \log A$.                     The unwinding function is also shown to be closely connected with the matrix                     sign function.                     An algorithm for computing the unwinding function based on the                     Schur--Parlett method with a special reordering is proposed.                     It is shown that matrix argument reduction using the function                     $\mathrm{mod}(A) = A-2\pi i\, \mathcal{U}(A)$,                     which has eigenvalues with imaginary parts in the interval                     $(-\pi,\pi]$ and for which ${\mathrm{e}}^A = {\mathrm{e}}^{\mathrm{mod}(A)}$,                     can give significant computational savings in the evaluation                     of the exponential by scaling and squaring algorithms. </content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 1, Page 88-109, January 2014. &lt;br/&gt; A new matrix function corresponding to the scalar unwinding number of                     Corless, Hare, and Jeffrey is introduced.                     This matrix unwinding function, $\mathcal{U}$, is shown to                     be a valuable tool for deriving                     identities involving the matrix logarithm and                     fractional matrix powers,                     revealing, for example, the                     precise relation between $\log A^\alpha$ and $\alpha \log A$.                     The unwinding function is also shown to be closely connected with the matrix                     sign function.                     An algorithm for computing the unwinding function based on the                     Schur--Parlett method with a special reordering is proposed.                     It is shown that matrix argument reduction using the function                     $\mathrm{mod}(A) = A-2\pi i\, \mathcal{U}(A)$,                     which has eigenvalues with imaginary parts in the interval                     $(-\pi,\pi]$ and for which ${\mathrm{e}}^A = {\mathrm{e}}^{\mathrm{mod}(A)}$,                     can give significant computational savings in the evaluation                     of the exponential by scaling and squaring algorithms. </rss:description>
      <dc:title>The Matrix Unwinding Function, with an Application to Computing the Matrix Exponential</dc:title>
      <dc:identifier>doi:10.1137/130920137</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 30 Jan 2014 08:00:00 GMT</dc:date> Mary Aprahamian  and  Nicholas J. Higham <prism:publicationName>The Matrix Unwinding Function, with an Application to Computing the Matrix Exponential</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>1</prism:number>
      <prism:startingPage>88</prism:startingPage>
      <prism:endingPage>109</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130920137</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130920137?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/120889848?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Differentiating the Method of Conjugate Gradients</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/120889848?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/1"&gt;Volume 35, Issue 1&lt;/a&gt;, Page 110-126, January 2014. &lt;br/&gt; The method of conjugate gradients (CG) is widely used for the iterative solution of large sparse  systems of equations $Ax=b$, where $A\in\Re^{n\times n}$ is symmetric positive definite. Let $x_k$ denote the $k$th iterate of CG. This is a nonlinear differentiable function of $b$. In this paper we obtain expressions for $J_k$, the Jacobian matrix of $x_k$ with respect to $b$. We use these expressions to obtain bounds on $\|J_k\|_2$, the spectral norm condition number of $x_k$, and discuss algorithms to compute or estimate $J_kv$ and $J_k^Tv$ for a given vector $v$.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 1, Page 110-126, January 2014. &lt;br/&gt; The method of conjugate gradients (CG) is widely used for the iterative solution of large sparse  systems of equations $Ax=b$, where $A\in\Re^{n\times n}$ is symmetric positive definite. Let $x_k$ denote the $k$th iterate of CG. This is a nonlinear differentiable function of $b$. In this paper we obtain expressions for $J_k$, the Jacobian matrix of $x_k$ with respect to $b$. We use these expressions to obtain bounds on $\|J_k\|_2$, the spectral norm condition number of $x_k$, and discuss algorithms to compute or estimate $J_kv$ and $J_k^Tv$ for a given vector $v$.</rss:description>
      <dc:title>Differentiating the Method of Conjugate Gradients</dc:title>
      <dc:identifier>doi:10.1137/120889848</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Tue, 04 Feb 2014 08:00:00 GMT</dc:date>Serge Gratton, David Titley-Peloquin, Philippe Toint, and Jean Tshimanga Ilunga<prism:publicationName>Differentiating the Method of Conjugate Gradients</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>1</prism:number>
      <prism:startingPage>110</prism:startingPage>
      <prism:endingPage>126</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/120889848</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/120889848?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130919490?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>The CP-Matrix Completion Problem</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130919490?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/1"&gt;Volume 35, Issue 1&lt;/a&gt;, Page 127-142, January 2014. &lt;br/&gt; A symmetric matrix $C$ is completely positive (CP) if there exists an entrywise nonnegative matrix $B$ such that $C=BB^T$. The CP-completion problem is to study whether we can assign values to the missing entries of a partial matrix (i.e., a matrix having unknown entries) such that the completed matrix is completely positive. We propose a semidefinite algorithm for solving general CP-completion problems and study its properties. When all of the diagonal entries are given, the algorithm can give a certificate if a partial matrix is not CP-completable, and it almost always gives a CP-completion if it is CP-completable. When diagonal entries are partially given, similar properties hold. Computational experiments are also presented to show how CP-completion problems can be solved.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 1, Page 127-142, January 2014. &lt;br/&gt; A symmetric matrix $C$ is completely positive (CP) if there exists an entrywise nonnegative matrix $B$ such that $C=BB^T$. The CP-completion problem is to study whether we can assign values to the missing entries of a partial matrix (i.e., a matrix having unknown entries) such that the completed matrix is completely positive. We propose a semidefinite algorithm for solving general CP-completion problems and study its properties. When all of the diagonal entries are given, the algorithm can give a certificate if a partial matrix is not CP-completable, and it almost always gives a CP-completion if it is CP-completable. When diagonal entries are partially given, similar properties hold. Computational experiments are also presented to show how CP-completion problems can be solved.</rss:description>
      <dc:title>The CP-Matrix Completion Problem</dc:title>
      <dc:identifier>doi:10.1137/130919490</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Tue, 04 Feb 2014 08:00:00 GMT</dc:date>Anwa Zhou and Jinyan Fan<prism:publicationName>The CP-Matrix Completion Problem</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>1</prism:number>
      <prism:startingPage>127</prism:startingPage>
      <prism:endingPage>142</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130919490</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130919490?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130911962?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>A New Analysis of Block Preconditioners for Saddle Point Problems</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130911962?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/1"&gt;Volume 35, Issue 1&lt;/a&gt;, Page 143-173, January 2014. &lt;br/&gt; We consider symmetric saddle point matrices. We analyze block preconditioners based on the knowledge of a good approximation for both the top left block and the Schur complement resulting from its elimination. We obtain bounds on the eigenvalues of the preconditioned matrix that depend only of the quality of these approximations, as measured by the related condition numbers. Our analysis applies to indefinite block diagonal preconditioners, block triangular preconditioners, inexact Uzawa preconditioners, block approximate factorization preconditioners, and a further enhancement of these preconditioners based on symmetric block Gauss--Seidel-type iterations. The analysis is unified and allows the comparison of these different approaches. In particular, it reveals that block triangular and inexact Uzawa preconditioners lead to identical eigenvalue distributions. These theoretical results are illustrated on the discrete Stokes problem. It turns out that the provided bounds allow one to localize accurately both real and nonreal eigenvalues. The relative quality of the different types of preconditioners is also as expected from the theory.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 1, Page 143-173, January 2014. &lt;br/&gt; We consider symmetric saddle point matrices. We analyze block preconditioners based on the knowledge of a good approximation for both the top left block and the Schur complement resulting from its elimination. We obtain bounds on the eigenvalues of the preconditioned matrix that depend only of the quality of these approximations, as measured by the related condition numbers. Our analysis applies to indefinite block diagonal preconditioners, block triangular preconditioners, inexact Uzawa preconditioners, block approximate factorization preconditioners, and a further enhancement of these preconditioners based on symmetric block Gauss--Seidel-type iterations. The analysis is unified and allows the comparison of these different approaches. In particular, it reveals that block triangular and inexact Uzawa preconditioners lead to identical eigenvalue distributions. These theoretical results are illustrated on the discrete Stokes problem. It turns out that the provided bounds allow one to localize accurately both real and nonreal eigenvalues. The relative quality of the different types of preconditioners is also as expected from the theory.</rss:description>
      <dc:title>A New Analysis of Block Preconditioners for Saddle Point Problems</dc:title>
      <dc:identifier>doi:10.1137/130911962</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 06 Feb 2014 08:00:00 GMT</dc:date>Yvan Notay<prism:publicationName>A New Analysis of Block Preconditioners for Saddle Point Problems</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>1</prism:number>
      <prism:startingPage>143</prism:startingPage>
      <prism:endingPage>173</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130911962</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130911962?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130922872?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>On Incremental Condition Estimators in the 2-Norm</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130922872?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/1"&gt;Volume 35, Issue 1&lt;/a&gt;, Page 174-197, January 2014. &lt;br/&gt; This paper deals with estimating the condition number of triangular matrices in the Euclidean norm. The two main incremental methods, based on the work of Bischof and the later work of Duff and Vömel, are compared. The paper presents new theoretical results revealing their similarities and differences. As typical in condition number estimation, there is no universal always-winning strategy, but theoretical and experimental arguments show that the clearly preferable approach is the algorithm of Duff and Vömel when appropriately applied to both the triangular matrix itself and its inverse. This leads to a highly accurate incremental condition number estimator.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 1, Page 174-197, January 2014. &lt;br/&gt; This paper deals with estimating the condition number of triangular matrices in the Euclidean norm. The two main incremental methods, based on the work of Bischof and the later work of Duff and Vömel, are compared. The paper presents new theoretical results revealing their similarities and differences. As typical in condition number estimation, there is no universal always-winning strategy, but theoretical and experimental arguments show that the clearly preferable approach is the algorithm of Duff and Vömel when appropriately applied to both the triangular matrix itself and its inverse. This leads to a highly accurate incremental condition number estimator.</rss:description>
      <dc:title>On Incremental Condition Estimators in the 2-Norm</dc:title>
      <dc:identifier>doi:10.1137/130922872</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Wed, 12 Feb 2014 08:00:00 GMT</dc:date>Jurjen Duintjer Tebbens and Miroslav Tůma<prism:publicationName>On Incremental Condition Estimators in the 2-Norm</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>1</prism:number>
      <prism:startingPage>174</prism:startingPage>
      <prism:endingPage>197</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130922872</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130922872?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/12088906X?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>An Algorithm for Finding an Optimal Projection of a Symmetric Matrix onto a Diagonal Matrix</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/12088906X?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/1"&gt;Volume 35, Issue 1&lt;/a&gt;, Page 198-224, January 2014. &lt;br/&gt; This work is motivated by two two-sided optimization problems that arise in atomic chemistry when one is looking for a minimal set of localized orbitals with prescribed occupation numbers, respectively, a prescribed total number of electrons. We first propose an optimal analytic solution of the first problem, and then show that an optimal solution of the second problem can be found by solving a convex quadratic programming problem with box constraints and $p$ unknowns. We prove that the latter problem is solved by the active-set method in at most $2p$ iterations. These investigations reveal that the optimal solutions of both problems are projections that are in $\mathcal{C}=\{Y\in\mathbb{R}^{n\times p}:Y^TY=I_p, Y^TN Y=\Delta\}$ for $N$ symmetric and $\Delta$ diagonal and that these solutions are generally nonunique. The question of how one can then select a particular solution out of the set $\mathcal{C}$ leads to the main problem of this paper: the optimization of an arbitrary smooth function whose minimizer describes the solution of interest over $\mathcal{C}$. To solve this problem, we first find that a slight modification of $\mathcal{C}$ is a Riemannian manifold for which geometric objects can be derived that are required to allow optimization over this manifold. Using these geometric tools we propose an augmented Lagrangian-based algorithm that guarantees global convergence to a stationary point of our main problem. The latter is shown by investigating when the LICQ is satisfied. Finally we compare this algorithm numerically with a similar algorithm that, however, does not apply these geometric tools and that is, to our knowledge, not guaranteed to converge. Our results show that our algorithm yields a significantly better performance.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 1, Page 198-224, January 2014. &lt;br/&gt; This work is motivated by two two-sided optimization problems that arise in atomic chemistry when one is looking for a minimal set of localized orbitals with prescribed occupation numbers, respectively, a prescribed total number of electrons. We first propose an optimal analytic solution of the first problem, and then show that an optimal solution of the second problem can be found by solving a convex quadratic programming problem with box constraints and $p$ unknowns. We prove that the latter problem is solved by the active-set method in at most $2p$ iterations. These investigations reveal that the optimal solutions of both problems are projections that are in $\mathcal{C}=\{Y\in\mathbb{R}^{n\times p}:Y^TY=I_p, Y^TN Y=\Delta\}$ for $N$ symmetric and $\Delta$ diagonal and that these solutions are generally nonunique. The question of how one can then select a particular solution out of the set $\mathcal{C}$ leads to the main problem of this paper: the optimization of an arbitrary smooth function whose minimizer describes the solution of interest over $\mathcal{C}$. To solve this problem, we first find that a slight modification of $\mathcal{C}$ is a Riemannian manifold for which geometric objects can be derived that are required to allow optimization over this manifold. Using these geometric tools we propose an augmented Lagrangian-based algorithm that guarantees global convergence to a stationary point of our main problem. The latter is shown by investigating when the LICQ is satisfied. Finally we compare this algorithm numerically with a similar algorithm that, however, does not apply these geometric tools and that is, to our knowledge, not guaranteed to converge. Our results show that our algorithm yields a significantly better performance.</rss:description>
      <dc:title>An Algorithm for Finding an Optimal Projection of a Symmetric Matrix onto a Diagonal Matrix</dc:title>
      <dc:identifier>doi:10.1137/12088906X</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 20 Feb 2014 08:00:00 GMT</dc:date>Rüdiger Borsdorf<prism:publicationName>An Algorithm for Finding an Optimal Projection of a Symmetric Matrix onto a Diagonal Matrix</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>1</prism:number>
      <prism:startingPage>198</prism:startingPage>
      <prism:endingPage>224</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/12088906X</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/12088906X?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130905010?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Robust Low-Rank Tensor Recovery: Models and Algorithms</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130905010?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/1"&gt;Volume 35, Issue 1&lt;/a&gt;, Page 225-253, January 2014. &lt;br/&gt; Robust tensor recovery plays an instrumental role in robustifying tensor decompositions for multilinear data analysis against outliers, gross corruptions, and missing values and has a diverse array of applications.  In this paper, we study the problem of robust low-rank tensor recovery in a convex optimization framework, drawing upon recent advances in robust principal component analysis and tensor completion.  We propose tailored optimization algorithms with global convergence guarantees for solving both the constrained and the Lagrangian formulations of the problem.  These algorithms are based on the highly efficient alternating direction augmented Lagrangian and accelerated proximal gradient methods.  We also propose a nonconvex model that can often improve the recovery results from the convex models.  We investigate the empirical recoverability properties of the convex and nonconvex formulations and compare the computational performance of the algorithms on simulated data.  We demonstrate through a number of real applications the practical effectiveness of this convex optimization framework for robust low-rank tensor recovery.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 1, Page 225-253, January 2014. &lt;br/&gt; Robust tensor recovery plays an instrumental role in robustifying tensor decompositions for multilinear data analysis against outliers, gross corruptions, and missing values and has a diverse array of applications.  In this paper, we study the problem of robust low-rank tensor recovery in a convex optimization framework, drawing upon recent advances in robust principal component analysis and tensor completion.  We propose tailored optimization algorithms with global convergence guarantees for solving both the constrained and the Lagrangian formulations of the problem.  These algorithms are based on the highly efficient alternating direction augmented Lagrangian and accelerated proximal gradient methods.  We also propose a nonconvex model that can often improve the recovery results from the convex models.  We investigate the empirical recoverability properties of the convex and nonconvex formulations and compare the computational performance of the algorithms on simulated data.  We demonstrate through a number of real applications the practical effectiveness of this convex optimization framework for robust low-rank tensor recovery.</rss:description>
      <dc:title>Robust Low-Rank Tensor Recovery: Models and Algorithms</dc:title>
      <dc:identifier>doi:10.1137/130905010</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 06 Mar 2014 08:00:00 GMT</dc:date>Donald Goldfarb and Zhiwei (Tony) Qin<prism:publicationName>Robust Low-Rank Tensor Recovery: Models and Algorithms</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>1</prism:number>
      <prism:startingPage>225</prism:startingPage>
      <prism:endingPage>253</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130905010</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130905010?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130905216?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>On the Inverse Symmetric Quadratic Eigenvalue Problem</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130905216?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/1"&gt;Volume 35, Issue 1&lt;/a&gt;, Page 254-278, January 2014. &lt;br/&gt; The detailed spectral structure of symmetric, algebraic, quadratic eigenvalue problems has been developed recently. In this paper we take advantage of these canonical forms to provide a detailed analysis of inverse problems of the following form: construct the coefficient matrices from the spectral data including the classical eigenvalue/eigenvector data and sign characteristics for the real eigenvalues. An orthogonality condition dependent on these signs plays a vital role in this construction. Special attention is paid to the cases when the leading and trailing coefficients of the quadratic matrix polynomial are prescribed to be positive definite.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 1, Page 254-278, January 2014. &lt;br/&gt; The detailed spectral structure of symmetric, algebraic, quadratic eigenvalue problems has been developed recently. In this paper we take advantage of these canonical forms to provide a detailed analysis of inverse problems of the following form: construct the coefficient matrices from the spectral data including the classical eigenvalue/eigenvector data and sign characteristics for the real eigenvalues. An orthogonality condition dependent on these signs plays a vital role in this construction. Special attention is paid to the cases when the leading and trailing coefficients of the quadratic matrix polynomial are prescribed to be positive definite.</rss:description>
      <dc:title>On the Inverse Symmetric Quadratic Eigenvalue Problem</dc:title>
      <dc:identifier>doi:10.1137/130905216</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 06 Mar 2014 08:00:00 GMT</dc:date>Peter Lancaster and Ion Zaballa<prism:publicationName>On the Inverse Symmetric Quadratic Eigenvalue Problem</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>1</prism:number>
      <prism:startingPage>254</prism:startingPage>
      <prism:endingPage>278</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130905216</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130905216?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/120895639?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Stability of Two Direct Methods for Bidiagonalization and Partial Least Squares</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/120895639?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/1"&gt;Volume 35, Issue 1&lt;/a&gt;, Page 279-291, January 2014. &lt;br/&gt; The partial least squares (PLS) method computes a sequence of approximate solutions $x_k \in {\cal K}_k(A^TA,A^Tb)$, $k = 1,2,\ldots\,$, to the least squares problem $\min_x\|Ax - b\|_2$. If carried out to completion, the method always terminates with the pseudoinverse solution $x^\dagger = A^\dagger b$. Two direct PLS algorithms are analyzed. The first uses the Golub--Kahan Householder algorithm for reducing $A$ to upper bidiagonal form. The second is the NIPALS PLS algorithm, due to Wold et al., which is based on rank-reducing orthogonal projections. The Householder algorithm is known to be mixed forward-backward stable. Numerical results are given, that support the conjecture that the NIPALS PLS algorithm shares this stability property. We draw attention to a flaw in some descriptions and implementations of this algorithm, related to a similar problem in Gram--Schmidt orthogonalization, that spoils its otherwise excellent stability. For large-scale sparse or structured problems, the iterative algorithm LSQR is an attractive alternative, provided an implementation with reorthogonalization is used.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 1, Page 279-291, January 2014. &lt;br/&gt; The partial least squares (PLS) method computes a sequence of approximate solutions $x_k \in {\cal K}_k(A^TA,A^Tb)$, $k = 1,2,\ldots\,$, to the least squares problem $\min_x\|Ax - b\|_2$. If carried out to completion, the method always terminates with the pseudoinverse solution $x^\dagger = A^\dagger b$. Two direct PLS algorithms are analyzed. The first uses the Golub--Kahan Householder algorithm for reducing $A$ to upper bidiagonal form. The second is the NIPALS PLS algorithm, due to Wold et al., which is based on rank-reducing orthogonal projections. The Householder algorithm is known to be mixed forward-backward stable. Numerical results are given, that support the conjecture that the NIPALS PLS algorithm shares this stability property. We draw attention to a flaw in some descriptions and implementations of this algorithm, related to a similar problem in Gram--Schmidt orthogonalization, that spoils its otherwise excellent stability. For large-scale sparse or structured problems, the iterative algorithm LSQR is an attractive alternative, provided an implementation with reorthogonalization is used.</rss:description>
      <dc:title>Stability of Two Direct Methods for Bidiagonalization and Partial Least Squares</dc:title>
      <dc:identifier>doi:10.1137/120895639</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Tue, 11 Mar 2014 07:00:00 GMT</dc:date>Ake Björck<prism:publicationName>Stability of Two Direct Methods for Bidiagonalization and Partial Least Squares</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>1</prism:number>
      <prism:startingPage>279</prism:startingPage>
      <prism:endingPage>291</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/120895639</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/120895639?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/120869432?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Subspace Methods for Computing the Pseudospectral Abscissa and the Stability Radius</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/120869432?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/1"&gt;Volume 35, Issue 1&lt;/a&gt;, Page 292-313, January 2014. &lt;br/&gt; The pseudospectral abscissa and the stability radius are well-established tools for quantifying the stability of a matrix under unstructured perturbations. Based on first-order eigenvalue expansions, Guglielmi and Overton [SIAM J. Matrix Anal. Appl., 32 (2011), pp. 1166--1192] recently proposed a linearly converging iterative method for computing the pseudospectral abscissa. In this paper, we propose to combine this method and its variants with subspace acceleration. Each extraction step computes the pseudospectral abscissa of a small rectangular matrix pencil, which is comparably cheap and guarantees monotonicity. We observe local quadratic convergence and prove local superlinear convergence of the resulting subspace methods. Moreover, these methods extend naturally to computing the stability radius. A number of numerical experiments demonstrate the robustness and efficiency of the subspace methods.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 1, Page 292-313, January 2014. &lt;br/&gt; The pseudospectral abscissa and the stability radius are well-established tools for quantifying the stability of a matrix under unstructured perturbations. Based on first-order eigenvalue expansions, Guglielmi and Overton [SIAM J. Matrix Anal. Appl., 32 (2011), pp. 1166--1192] recently proposed a linearly converging iterative method for computing the pseudospectral abscissa. In this paper, we propose to combine this method and its variants with subspace acceleration. Each extraction step computes the pseudospectral abscissa of a small rectangular matrix pencil, which is comparably cheap and guarantees monotonicity. We observe local quadratic convergence and prove local superlinear convergence of the resulting subspace methods. Moreover, these methods extend naturally to computing the stability radius. A number of numerical experiments demonstrate the robustness and efficiency of the subspace methods.</rss:description>
      <dc:title>Subspace Methods for Computing the Pseudospectral Abscissa and the Stability Radius</dc:title>
      <dc:identifier>doi:10.1137/120869432</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 20 Mar 2014 07:00:00 GMT</dc:date>Daniel Kressner and Bart Vandereycken<prism:publicationName>Subspace Methods for Computing the Pseudospectral Abscissa and the Stability Radius</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>1</prism:number>
      <prism:startingPage>292</prism:startingPage>
      <prism:endingPage>313</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/120869432</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/120869432?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
   <rss:item rdf:about="http://epubs.siam.org/doi/abs/10.1137/130944552?ai=s7&amp;mi=3ezuvv&amp;af=R">
      <rss:title>Erratum: Preconditioning Stochastic Galerkin Saddle Point Systems</rss:title>
      <rss:link>http://epubs.siam.org/doi/abs/10.1137/130944552?ai=s7&amp;mi=3ezuvv&amp;af=R</rss:link>
      <content:encoded>SIAM Journal on Matrix Analysis and Applications, &lt;a href="/toc/sjmael/35/1"&gt;Volume 35, Issue 1&lt;/a&gt;, Page 314-315, January 2014. &lt;br/&gt; The purpose of this brief erratum is to correct a mistake in two lines of the proof of Lemma 5.3 in [SIAM J. Matrix Anal. Appl., 31 (2010), pp. 2813--2840]. This mistake does not affect the validity of Lemma 5.3.</content:encoded>
      <rss:description>SIAM Journal on Matrix Analysis and Applications, Volume 35, Issue 1, Page 314-315, January 2014. &lt;br/&gt; The purpose of this brief erratum is to correct a mistake in two lines of the proof of Lemma 5.3 in [SIAM J. Matrix Anal. Appl., 31 (2010), pp. 2813--2840]. This mistake does not affect the validity of Lemma 5.3.</rss:description>
      <dc:title>Erratum: Preconditioning Stochastic Galerkin Saddle Point Systems</dc:title>
      <dc:identifier>doi:10.1137/130944552</dc:identifier>
      <dc:source>SIAM Journal on Matrix Analysis and Applications</dc:source>
      <dc:date>Thu, 27 Mar 2014 07:00:00 GMT</dc:date>Catherine E. Powell and Elisabeth Ullmann<prism:publicationName>Erratum: Preconditioning Stochastic Galerkin Saddle Point Systems</prism:publicationName>
      <prism:volume>35</prism:volume>
      <prism:number>1</prism:number>
      <prism:startingPage>314</prism:startingPage>
      <prism:endingPage>315</prism:endingPage>
      <prism:coverDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDate>
      <prism:coverDisplayDate>Wed, 01 Jan 2014 08:00:00 GMT</prism:coverDisplayDate>
      <prism:doi>10.1137/130944552</prism:doi>
      <prism:url>http://epubs.siam.org/doi/abs/10.1137/130944552?ai=s7&amp;mi=3ezuvv&amp;af=R</prism:url>
      <prism:copyright>© 2014, Society for Industrial and Applied Mathematics</prism:copyright>
   </rss:item>
</rdf:RDF>